{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hired-merchandise",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Notes from https://learn.datacamp.com/courses/big-data-fundamentals-with-pyspark\n",
    "\n",
    "\n",
    "\n",
    "# Pyspark distributes data and computations over multiple nodes \n",
    "# spark can be run on local machine (local mode) or cluster mode (on predefined clusters)\n",
    "# Spark Shell = REPL for interacting with Spark directly. PySpark shell is one implementation\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pointed-machine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.2\n",
      "local\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(\"local\", \"My App\")  # sets context\n",
    "\n",
    "\n",
    "print(sc.version)\n",
    "print(sc.master)  # URL of cluster for sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-softball",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "involved-grant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 4, 6, 8]\n",
      "[True, False, True, False]\n",
      "[1, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2, 4, 6, 8]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## lambda syntax:\n",
    "# lambda arguments: expression\n",
    "# eg lambda x: x*2\n",
    "\n",
    "vals = [1, 2, 3, 4]\n",
    "print(list(map(lambda x:x*2, vals)))  # map lambda to all vals in list (returns map obj which we convert to list)\n",
    "\n",
    "print(list(map(lambda x:(x%2 != 0), vals)))  # func returning true/false\n",
    "print(list(filter(lambda x:(x%2 != 0), vals)))  # filter() goes one further than map() and excludes vals where false\n",
    "\n",
    "print([x*2 for x in vals])    # nicer way of doing same thing as map()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "brilliant-mobility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[52] at readRDDFromFile at PythonRDD.scala:274\n",
      "ParallelCollectionRDD[53] at readRDDFromFile at PythonRDD.scala:274\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hah', 'ahaadf', 'hahaoo', 'a']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RDD = main pyspark distributed dataset\n",
    "\n",
    "rdd_numeric = sc.parallelize([1, 2, 3, 4])  # parallelize() makes RDD from list or text\n",
    "print(rdd_numeric)\n",
    "\n",
    "rdd_text = sc.parallelize('hah aha hah a')  # parallelize() makes RDD from list or text\n",
    "rdd_text2 = sc.parallelize(\"hah ahaadf hahaoo a\") \n",
    "\n",
    "print(type(rdd_text))\n",
    "\n",
    "# sc.textFile('file.txt')   # to read from txr fiel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "going-flooring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations = create new RDDs\n",
    "# transformations are done using lazy evaluation\n",
    "\n",
    "# key transformations: map(), filter(), flatMap(), union()\n",
    "\n",
    "rdd_numeric.map(lambda x: x**2)\n",
    "rdd_numeric_filtered =rdd_numeric.filter(lambda x: x > 2)\n",
    "\n",
    "flat1 = rdd_text.flatMap(lambda x: x.split('a'))  # flatMap() can return multiple values for each input in a longer list-like\n",
    "flat2 = rdd_text2.flatMap(lambda x: x.split(\" \"))  # split seems to return list of individual chars() in pyspark\n",
    "                                    # but doesnt in base py\n",
    "\n",
    "flat_combined = flat1.union(flat2)  # concatenates and removes duplicates, just as union() in SQL does\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "nominated-clarity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h', '', '', 'h', ' ', '', '', 'h', '', '', ' ', 'h', '', '', 'h', ' ', '', '']\n",
      "ParallelCollectionRDD[53] at readRDDFromFile at PythonRDD.scala:274\n",
      "[3, 4]\n",
      "3\n",
      "['h', '', '', 'h', ' ', '', '']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# actions = computations on RDDs\n",
    "\n",
    "# key actions: collect(), take(N), reduce(), count()\n",
    "print(flat1.collect())\n",
    "print(rdd_text)\n",
    "print(rdd_numeric_filtered.collect())\n",
    "\n",
    "print(rdd_numeric_filtered.first())  # shows first val in sequence\n",
    "print(flat_combined.take(7))       # first N vals in sequence \n",
    "\n",
    "print(rdd_numeric_filtered.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-bicycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# got up to this video: \n",
    "# https://campus.datacamp.com/courses/big-data-fundamentals-with-pyspark/programming-in-pyspark-rdds?ex=8\n",
    "\n",
    "df = sparl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-dictionary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-canyon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-principle",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-contact",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-steal",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-bunch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-ceramic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-decline",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-wallace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-honey",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "studied-domestic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
