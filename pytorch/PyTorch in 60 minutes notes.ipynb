{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "nonprofit-ability",
   "metadata": {},
   "outputs": [],
   "source": [
    "# notes from https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "\n",
    "\n",
    "\n",
    "## Packages it recommends:\n",
    "# Text: NLTK (text wrangling), SpaCy (nlp models inc transfer learning from transformers - looks cutting edge)\n",
    "# audio: scipy, librosa (music and audio analysis)\n",
    "# images: Pillow (image wrangling), OpenCV (comp vision models)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# from playing with options on pytorch site it looks like mac doesnt get CUDA (GPU acceleration), \n",
    "# tho the accompanying text on the site suggests it does, so may well be I'm missing something\n",
    "# https://pytorch.org/get-started/locally/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imposed-harvard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# operation on np array linked to torch tensor updates torch tensor values too\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out = a)\n",
    "print(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "curious-northern",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1, 2],[3, 4]]\n",
    "x_data = torch.tensor(data)\n",
    "x_data.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "restricted-right",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4218, 0.8228, 0.6627],\n",
      "         [0.4727, 0.5870, 0.1737],\n",
      "         [0.4094, 0.9452, 0.5087]],\n",
      "\n",
      "        [[0.1408, 0.6108, 0.5357],\n",
      "         [0.4517, 0.9523, 0.5667],\n",
      "         [0.3393, 0.2441, 0.3165]]])\n",
      "torch.Size([2, 3, 3])\n",
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "shape = (2, 3, 3)\n",
    "x = torch.rand(shape) # make arrays of random values, similar to np.random.uniform\n",
    "print(x)\n",
    "print(x.shape)\n",
    "print(x.dtype)\n",
    "print(x.device)  # where tensor is stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "suspected-desire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU no available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():   # We move our tensor to the GPU if available\n",
    "  x = x.to('cuda')\n",
    "else:\n",
    "    print('GPU no available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "scenic-warner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9523, 0.5667],\n",
      "        [0.2441, 0.3165]])\n",
      "tensor([[0.9523, 0.5667, 0.9523, 0.5667],\n",
      "        [0.2441, 0.3165, 0.2441, 0.3165]])\n",
      "tensor([[0.7821, 0.4966],\n",
      "        [0.2723, 0.5283]])\n",
      "tensor([[0.7448, 0.2814],\n",
      "        [0.0665, 0.1672]])\n",
      "tensor([[0.7448, 0.2814],\n",
      "        [0.0665, 0.1672]])\n"
     ]
    }
   ],
   "source": [
    "print(x[1, 1:, 1:])  # slicing is same as numpy\n",
    "\n",
    "print(torch.cat([x[1, 1:, 1:], x[1, 1:, 1:]], dim = 1))   # concat arrays along chosen dimension\n",
    "\n",
    "\n",
    "### Two ways of doing element-wise multiplication\n",
    "rand_tensor = torch.rand(2, 2)\n",
    "print(rand_tensor)\n",
    "print(x[1, 1:, 1:] * rand_tensor)\n",
    "print(x[1, 1:, 1:].mul(rand_tensor))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "timely-mixture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8991, 0.7724],\n",
      "        [0.2771, 0.2884]])\n",
      "tensor([[0.8991, 0.7724],\n",
      "        [0.2771, 0.2884]])\n"
     ]
    }
   ],
   "source": [
    "### matrix multiplication\n",
    "print(x[1, 1:, 1:] @ rand_tensor)\n",
    "print(x[1, 1:, 1:].matmul(rand_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "minimal-lighting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7821, 0.4966],\n",
      "        [0.2723, 0.5283]])\n",
      "tensor([[2.7821, 2.4966],\n",
      "        [2.2723, 2.5283]])\n"
     ]
    }
   ],
   "source": [
    "print(rand_tensor)\n",
    "rand_tensor.add_(2)  # function that ends in _ performs inplace operation\n",
    "print(rand_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "increased-holocaust",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /Users/apple/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad8f0bdfbc44a2b986feefeb532a284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)   # downloading pre-trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "sensitive-melbourne",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.getsizeof(model)  # seems unlikely it's only 48 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "stupid-strip",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-507.5631, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## applying model to random data: more here https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html\n",
    "data = torch.rand(1, 3, 64, 64)    # single image I think, but not sure\n",
    "labels = torch.rand(1, 1000)\n",
    "\n",
    "prediction = model(data)  # forward pass\n",
    "\n",
    "loss = (prediction - labels).sum()\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "treated-latin",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-60680837a514>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# backward() begins backpropagation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "loss.backward() # backward() begins backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "signal-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9) # define optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "revised-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim.step() # one iteration of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "starting-petroleum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.01\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(optim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "digital-phoenix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define ConvNet from:\n",
    "# https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    "class Net(nn.Module):  # nn.Module is base class for all pytorch neural nets\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()   # this means __init__ calls the __init__ function of the superclass\n",
    "                            # in this case, nn.Module\n",
    "            \n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # 6 in and 16 out channels, 5x5 convolution\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5*5 from image dimension, 120 nodes\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)        # outputs 10 values\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"define forward pass nn architecutre, calling processes set out above\"\"\"\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        \n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)         # no activation function here\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "collective-blackberry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1302, -0.1424,  0.1806,  0.1944,  0.0090],\n",
       "         [ 0.1023,  0.1441, -0.0005, -0.0689,  0.1372],\n",
       "         [-0.0660,  0.1985, -0.1321,  0.1991, -0.1594],\n",
       "         [-0.1626, -0.1843,  0.1169, -0.0697,  0.0135],\n",
       "         [-0.1068, -0.0049,  0.0890, -0.1238, -0.1596]]],\n",
       "       grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = list(net.parameters()) # getting model's parameters\n",
    "params[0][0]   # first of several arrays of learned weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "opposed-queensland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0370, -0.1661, -0.0757, -0.1018,  0.0398, -0.1242, -0.0264,  0.0217,\n",
      "          0.1132, -0.0134]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)  # single data input\n",
    "out = net(input)              # apply model to data\n",
    "print(out)                  # non-activated (dont sum to 1) outputs for each of 10 possible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "spectacular-newspaper",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "naughty-pixel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0370, -0.1661, -0.0757, -0.1018,  0.0398, -0.1242, -0.0264,  0.0217,\n",
       "          0.1132, -0.0134]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.Tensor = ndarray with support for funcs like backward(). Contain's tensor's gradient info \n",
    "# nn.Module = NN superclass to inherit from \n",
    "# nn.Parameter = kind of tensor, automatically registered as a param when assigned as a Module attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "loose-column",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9018, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# torch.nn includes a few loss functions\n",
    "# \n",
    "\n",
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()  # choose loss func\n",
    "\n",
    "loss = criterion(output, target)  # apply loss func to single data (1*10 array as 10 possible classes)\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "apparent-presence",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MseLossBackward at 0x14e755c70>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.grad_fn  # says this shows the computation graph it went through, but cant see it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ongoing-format",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddmmBackward object at 0x14e6cbdc0>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn.next_functions[0][0])  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "binary-fever",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-612bdcd8d29b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# it only lets you run this once, but it runs instantly so unsure if it's running the whole grad desc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'conv1.bias.grad after backward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()    # it only lets you run this once, but it runs instantly so unsure if it's running the full grad desc\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "junior-gnome",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create your optimizer\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update\n",
    "\n",
    "\n",
    "\n",
    "### the above is a better version than the below, whcih is fine if you're using normal gradient descent\n",
    "### without an optimiser\n",
    "#learning_rate = 0.01\n",
    "#for f in net.parameters():\n",
    "#    f.data.sub_(f.grad.data * learning_rate)  # subtracts from f inplace, effectively: \n",
    "                #f = f - derivative_weight *learn_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-uncle",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
