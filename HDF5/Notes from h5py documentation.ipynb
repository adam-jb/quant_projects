{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "present-applicant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/quant_projects/HDF5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Notes from https://docs.h5py.org/en/stable/quick.html\n",
    "\n",
    "# h5py lets you store huge amounts of numerical data, and easily manipulate that data from NumPy.  \n",
    "# For example, you can slice into multi-terabyte datasets stored on disk, as if they were real NumPy arrays\n",
    "\n",
    "# to save more space can compress hdf5 files. Instructions here:\n",
    "# https://www.christopherlovell.co.uk/blog/2016/04/27/h5py-intro.html\n",
    "\n",
    "# hdf5 is optimised for very large datasets\n",
    "\n",
    "\n",
    "import h5py\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "appointed-tutorial",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'datasets' are like numpy arrays\n",
    "# 'groups' are containers which hold 'datasets' and act like dictionaries (eg each file has a key)\n",
    "\n",
    "# keys of 'groups' do follow a file structure\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "important-singles",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'f' is context manager object\n",
    "f = h5py.File(\"mytestfile.hdf5\", \"w\")  # makes a new hdf5 file if not there ('w' for write mode:\n",
    "                        # 'a' for append often more appropriate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "sharing-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_data = np.random.randn(100).reshape(50, 2) # create_dataset doesnt work with this: accepts tuple on line below \n",
    "fake_data = (100, )\n",
    "dset = f.create_dataset(\"mydataset\", fake_data, dtype='i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "permanent-cache",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mydataset'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dset.name   # shows where this dataset sits in the hdf5 file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "external-region",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.name    # 'f' is itself a group, at the top of the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "drawn-disease",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('mytestfile.hdf5', 'a')  # opening file in append mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "plain-saskatchewan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/subgroup/another_dataset'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding subgroup and dataset to that subgroup\n",
    "grp = f.create_group(\"subgroup\") \n",
    "dset2 = grp.create_dataset(\"another_dataset\", (50,), dtype='f')\n",
    "dset2.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "opened-economy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/subgroup2/dataset_three'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adding new subgroup implicitly when adding a dataset\n",
    "dset3 = f.create_dataset('subgroup2/dataset_three', (10,), dtype='i')\n",
    "dset3.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "danish-intensity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mydataset\n",
      "subgroup\n",
      "subgroup2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(name) for name in f]    # view all subgroups (top of folder, non-recursive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acknowledged-spoke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mydataset\n",
      "subgroup\n",
      "subgroup/another_dataset\n",
      "subgroup2\n",
      "subgroup2/dataset_three\n"
     ]
    }
   ],
   "source": [
    "f.visit(lambda x:print(x))    # view all files recursively using visit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "revised-breakfast",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['mydataset', 'subgroup', 'subgroup2']>\n",
      "ValuesViewHDF5(<HDF5 file \"mytestfile.hdf5\" (mode r+)>)\n",
      "ItemsViewHDF5(<HDF5 file \"mytestfile.hdf5\" (mode r+)>)\n"
     ]
    }
   ],
   "source": [
    "print(f.keys())\n",
    "print(f.values())\n",
    "print(f.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "steady-oriental",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['temperature']>\n",
      "99.5\n"
     ]
    }
   ],
   "source": [
    "# can attach attributes (metadata) to individual datasets. These are structured like a dict\n",
    "dset.attrs['temperature'] = 99.5\n",
    "print(dset.attrs.keys())        # view keys\n",
    "print(dset.attrs['temperature'])  # get val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "micro-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()   # close the file: writes changes to disk (until now the changes haven't been saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "acquired-copper",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mydataset\n",
      "subgroup\n",
      "subgroup/another_dataset\n",
      "subgroup2\n",
      "subgroup2/dataset_three\n"
     ]
    }
   ],
   "source": [
    "# opening hdf5 file again\n",
    "f = h5py.File('mytestfile.hdf5', 'a')\n",
    "f.visit(lambda x:print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "annoying-membership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"numpy_test2\": shape (100, 20), type \"<f8\">"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add numpy array\n",
    "data_to_write = np.random.random(size=(100,20)) \n",
    "f.create_dataset(\"numpy_test2\",  data=data_to_write, maxshape=(None,None))  \n",
    "            # maxshape(None,None) ensures no limit on size of created dataset (default is unclear from \n",
    "            # docs and may be unlimited anyway)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "outer-customs",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 20)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# extract data as a numpy array\n",
    "data_output = f['numpy_test2'][:]\n",
    "print(data_output.shape)\n",
    "print(type(data_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "considered-heather",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 20)\n",
      "(450, 20)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[6.62907245e-01 4.85871072e-01 9.05875034e-01 3.81596814e-01\n",
      "  8.36464085e-01 9.94201460e-01 8.01879593e-01 8.59785088e-01\n",
      "  6.09562413e-01 3.35170721e-02 1.84888052e-01 2.53130979e-01\n",
      "  8.33279254e-03 4.75287368e-01 2.82348758e-01 7.66505240e-02\n",
      "  5.78233168e-01 4.59446264e-01 3.54505164e-01 9.32939336e-01]\n",
      " [2.82860724e-01 4.17865141e-02 5.92793606e-01 6.10683886e-05\n",
      "  2.11387466e-01 1.97272352e-01 3.19472313e-01 3.22475727e-01\n",
      "  1.93800500e-02 2.90371766e-02 2.93951089e-01 8.77412052e-01\n",
      "  6.32937861e-01 3.61012209e-01 6.23770265e-01 7.95583293e-01\n",
      "  9.46753998e-01 1.00695129e-02 3.11110836e-01 5.29381822e-01]]\n"
     ]
    }
   ],
   "source": [
    "# appending np array to existing h5py dataset\n",
    "new_array_to_append = np.random.random(size=(50,20)) \n",
    "print(f[\"numpy_test2\"].shape)  # shape before\n",
    "\n",
    "# adding empty rows\n",
    "f[\"numpy_test2\"].resize((f[\"numpy_test2\"].shape[0] + new_array_to_append.shape[0]), axis = 0)\n",
    "\n",
    "print(f[\"numpy_test2\"].shape)  # shape after adding more rows\n",
    "print(f['numpy_test2'][-2:])   # viewing bottom 2 rows before they're populated\n",
    "\n",
    "# putting values in those rows\n",
    "f[\"numpy_test2\"][-new_array_to_append.shape[0]:] = new_array_to_append\n",
    "print(f['numpy_test2'][-2:])   # viewing bottom 2 rows now they're populated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-radiation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "referenced-municipality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all the above hasn't been saved on the .hdf5 file on disk yet\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "patent-adolescent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4514  0.7266  0.565   0.1891 ]\n",
      " [0.10846 0.1633  0.05826 0.7046 ]\n",
      " [0.09766 0.0739  0.4036  0.0392 ]\n",
      " [0.404   0.911   0.98    0.628  ]]\n"
     ]
    }
   ],
   "source": [
    "# could use with: another way to read/write/delete/manipulate files\n",
    "data_to_write = np.random.random(size=(4,4)) \n",
    "\n",
    "with h5py.File(\"mytestfile.hdf5\", \"a\") as f:\n",
    "    del f['numpy_f2_floats']              # delete existing dataset of this name, so can overwrite in line below\n",
    "    f.create_dataset(\"numpy_f2_floats\",  data=data_to_write, maxshape=(None,None), dtype='f2')\n",
    "    print(f[\"numpy_f2_floats\"][:])\n",
    "    \n",
    "# don't need to close() when using 'with'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "vocational-republican",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mydataset\n",
      "numpy_f2_floats\n",
      "numpy_i2_integers\n",
      "numpy_test\n",
      "numpy_test2\n",
      "subgroup\n",
      "subgroup/another_dataset\n",
      "subgroup2\n",
      "subgroup2/dataset_three\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"mytestfile.hdf5\", \"a\") as f:\n",
    "    f.visit(lambda x:print(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "endangered-instrumentation",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'Dataset' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-64dbb88b328f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mytestfile.hdf5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'numpy_f2_floats'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'Dataset' and 'float'"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"mytestfile.hdf5\", \"a\") as f:\n",
    "    print(f['numpy_f2_floats'] > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "blond-tuner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"mytestfile.hdf5\", \"a\") as f:\n",
    "    del f['autochunk'] \n",
    "    dset = f.create_dataset(\"autochunk\", (4, 4), chunks=True)  # add autochunk param for h5py to automatically\n",
    "                                    # chunk data for you\n",
    "    print(f['autochunk'][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "generous-minority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7266 0.565  0.7046 0.911  0.98   0.628 ]\n"
     ]
    }
   ],
   "source": [
    "# supports numpy-style slicing for filters\n",
    "# Docs warn this gets slow with masks over 1000 values in length (ref: https://docs.h5py.org/en/stable/high/dataset.html#reading-writing-data)\n",
    "with h5py.File(\"mytestfile.hdf5\", \"a\") as f:\n",
    "    vals = f['numpy_f2_floats'][:] > 0.5\n",
    "    print(f['numpy_f2_floats'][vals])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "reverse-tomorrow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4514 0.7266 0.565  0.1891]\n",
      " [0.404  0.911  0.98   0.628 ]]\n"
     ]
    }
   ],
   "source": [
    "# selecting rows based on value in the first column\n",
    "with h5py.File(\"mytestfile.hdf5\", \"a\") as f:\n",
    "    vals = f['numpy_f2_floats'][:,0] > 0.4\n",
    "    vals = f['numpy_f2_floats'][:][vals]\n",
    "    print(vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-sympathy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Think there should be a way to filter giant dataset by a particular column (this seems\n",
    "# aligned with what hdf5 is optimised to do)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-security",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-lightning",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
