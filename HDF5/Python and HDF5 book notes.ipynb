{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "capable-temple",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Notes from O'Reilly Python and HDF5 book\n",
    "\n",
    "\n",
    "# note at end of this article on loading hdf5 into pytorch:\n",
    "# https://blade6570.github.io/soumyatripathy/hdf5_blog.html\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "from timeit import timeit\n",
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-satellite",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = np.random.random(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "tracked-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h5py uses C\n",
    "# B-tree: tree where each branch connects to nodes with 2+ children\n",
    "    # (instead of binary-tree where branch has exactly 2 children)\n",
    "    \n",
    "# 'groups' are stored in a B-tree, giving them the hierarchical format\n",
    "\n",
    "# suggests just use h5py as pacakge does it (not optimising further) unless you reallllly need to (which\n",
    "# I probably won't)\n",
    "\n",
    "# context managers set up and teardown resources (such as file connections). Generally for \n",
    "# more computationally expensive resources\n",
    "\n",
    "# driver types:\n",
    "# Core driver, stores all data in memory\n",
    "# f = h5py.File(\"name.hdf5\", driver=\"core\")   # to set context manager with core specified\n",
    "\n",
    "# Family driver, splits data into chunks and one chunk in memory at a time\n",
    "# f = h5py.File(\"family.hdf5\", driver=\"family\", memb_size=1024**3)  # 1gb at a time\n",
    "\n",
    "# mpio driver: for accessing data from many cores at the same time (related to 'Parallel HDF5')\n",
    "\n",
    "# 'locality': when reading off disk it's faster when the data is all stored as closely as possible\n",
    "\n",
    "# 'chunking': specify n-dimensional shape that fits you access pattern\n",
    "\n",
    "# chunks can be edited (compressed or decompressed) on their way out of retrival \n",
    "# dont need to do anything different for a chunked dataset except specify that it\n",
    "# chunked originally\n",
    "\n",
    "# says chunks over 1mb is bad idea as this is limit for it to use fast in-memory 'chunk cache'\n",
    "\n",
    "# 'filter pipeline' = series operations performed on a chunk before it's written\n",
    "\n",
    "# says rare for an application to spend most of its time compressing or decompressing data, \n",
    "# so try not to get carried away with speed testing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "variable-cherry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 30, 80)\n",
      "gzip\n"
     ]
    }
   ],
   "source": [
    "# set chunks=True to enable automatic chunking, where it decides chunk size for you\n",
    "dse2t = f.create_dataset(\"Images2d\", (100,480,640), 'f', chunks=True, compression='gzip')\n",
    "print(dse2t.chunks)\n",
    "print(dse2t.compression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "perfect-constitutional",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, 30, 80)\n",
      "gzip\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# set shuffle=True to enable shuffle prior to compression, saving space\n",
    "dse2t = f.create_dataset(\"Images2d_2\", (100,480,640), 'f', chunks=True, compression='gzip', shuffle=True)\n",
    "print(dse2t.chunks)\n",
    "print(dse2t.compression)\n",
    "print(dse2t.shuffle)\n",
    "\n",
    "# time to perform shuffle is negligible compared to compression, so cant see why you wouldnt do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LZF compression is a good alternative to gzip if working solely on python, and you dont mind\n",
    "# larger file size in exchange for faster compression/decompression\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dangerous-transport",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# FLETCHER32 = 32bit implementation of fletcher's checksum\n",
    "print(dse2t.fletcher32)\n",
    "\n",
    "# to enable fletcher32 checksum\n",
    "# dset = myfile.create_dataset(\"Data2\", (1000,), fletcher32=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flush-register",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the BLOSC compressor used by the PyTables project is highly tuned for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excessive-geometry",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "august-holder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n"
     ]
    }
   ],
   "source": [
    "# storing float64 data as float32 (converting) in hdf5\n",
    "bigdata = np.ones((100,1000))\n",
    "with h5py.File('big2.hdf5','w') as f2:\n",
    "    f2.create_dataset('big', data=bigdata, dtype=np.float32)\n",
    "\n",
    "f2 = h5py.File(\"big2.hdf5\")\n",
    "print(f2['big'].dtype)\n",
    "\n",
    "f2.close()   # wouldnt expect to need this but file doesnt seem to close without it, even with 'with' above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "numeric-edmonton",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dset4 = f[\"SubGroup/Dataset4\"] # Right\n",
    "#dset4 = f[\"SubGroup\"][\"Dataset4\"]   # works but inefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "needed-twist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P8020078.JPG', 'P8020087.JPG', 'P8110178.JPG', 'P7310037.JPG', 'P8100169.JPG']\n"
     ]
    }
   ],
   "source": [
    "# load australia photos into h5py\n",
    "fpath = Path('Python and HDF5 notes.ipynb').absolute()\n",
    "pathway = os.path.dirname(fpath)\n",
    "\n",
    "file_up_one_level = os.path.split(pathway)[0]\n",
    "os.chdir(file_up_one_level  + '/australia_photos')\n",
    "print(os.listdir()[:5])\n",
    "\n",
    "\n",
    "f = h5py.File(\"australia.hdf5\", 'w-')   # w- makes new file but errors on attempt to overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "meaningful-ranch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make subgroup to store images\n",
    "f = h5py.File(\"australia.hdf5\", 'a')\n",
    "subgroup = f.create_group(\"aus_iages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "worldwide-holiday",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/aus_iages\" (0 members)>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(subgroup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "married-musical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2272, 1704, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1620, 2221, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(2272, 1704, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(2272, 1704, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1619, 2217, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1544, 2187, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1588, 2264, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(2272, 1704, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1557, 2200, 3)\n",
      "(1608, 2235, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(1704, 2272, 3)\n",
      "(2272, 1704, 3)\n"
     ]
    },
    {
     "ename": "UnidentifiedImageError",
     "evalue": "cannot identify image file '.DS_Store'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-dc5d0664b814>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'australia.hdf5'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mimg_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mobj_name_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   2941\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maccept_warnings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2942\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2943\u001b[0;31m     raise UnidentifiedImageError(\n\u001b[0m\u001b[1;32m   2944\u001b[0m         \u001b[0;34m\"cannot identify image file %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m     )\n",
      "\u001b[0;31mUnidentifiedImageError\u001b[0m: cannot identify image file '.DS_Store'"
     ]
    }
   ],
   "source": [
    "# now load the photos into hdf5\n",
    "files = os.listdir()\n",
    "\n",
    "with h5py.File('australia.hdf5','a') as f2:\n",
    "    for file_path in files:\n",
    "        img_np = np.array(Image.open(file_path))\n",
    "        print(img_np.shape)\n",
    "        obj_name_new = file_path.split(',', 1)[0]\n",
    "        img_name = \"/aus_iages/\" + file_path\n",
    "        f2.create_dataset(img_name, data=img_np, dtype=np.float32, shuffle=True,chunks=True,compression='gzip')\n",
    "\n",
    "# this appears to be shockingly ineffienct to store vs jpeg (at least 6mb vs just under 1mb per image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-bibliography",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevant-printer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-union",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
