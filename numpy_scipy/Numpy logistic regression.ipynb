{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-sight",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## logistic regression in numpy, using arrays that will make extending it to a FFNN easier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# image input is 512*512*3. Unravel these to 1darray of length L\n",
    "# then put in array of training data, of dimensions L by m (where m is size of training group)\n",
    "# (a column for each datum)\n",
    "# and Y is 1*y matrix of labels\n",
    "\n",
    "# uses lower case m for training group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### logistic regression:\n",
    "# y = sigmoid(W^T*x + b)    # sigmoid of linear weights\n",
    "\n",
    "\n",
    "# assume all numpy functions are vectorised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "social-trash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 5\n",
      "(150, 5)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = pd.read_csv('https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv')\n",
    "data = data.to_numpy()\n",
    "\n",
    "\n",
    "# unique, counts = np.unique(data[:, 4], return_counts=True)\n",
    "setosa_idx = data[:, 4] == 'setosa'  # simplify species column to binary\n",
    "data[:, 4] = 1   # not setosa\n",
    "data[setosa_idx, 4] = 0   # setosa\n",
    "\n",
    "\n",
    "# data = np.hstack([np.array([1]*150).reshape(150,1), data])  # adding intercept column\n",
    "\n",
    "\n",
    "idx = np.random.rand(*data.shape).argsort(axis=0) # randomising order for test/train split\n",
    "data = np.take_along_axis(data,idx,axis=0)\n",
    "\n",
    "\n",
    "print(*data.shape)  # return values outside tuple\n",
    "print(data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "temporal-organizer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 120)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input = data[:120, :4].T   # transpose to row for each feature and column for each value: faster calcs\n",
    "train_labs = data[:120, 4:].T\n",
    "\n",
    "test_input = data[120:, :4].T\n",
    "test_labs = data[120:, 4:].T\n",
    "\n",
    "train_input.astype(np.float)\n",
    "test_input.astype(np.float)\n",
    "\n",
    "\n",
    "train_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "valuable-aircraft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "object\n"
     ]
    }
   ],
   "source": [
    "print(train_input.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "electric-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(y, y_hat):   \n",
    "    \"\"\"Logistic regression cost function\n",
    "    y = actual\n",
    "    y_hat = predicted\n",
    "    \n",
    "    If y=1 the 2nd part goes to zero. It rewards a large y_hat (ie, close to 1)\n",
    "    If y=0 it rewards a small y_hat value (ie, close to 0, as you'd expect)\n",
    "    \n",
    "    More on cost function here: \n",
    "    https://www.coursera.org/learn/neural-networks-deep-learning/lecture/yWaRd/logistic-regression-cost-function\n",
    "    \"\"\"\n",
    "    m = y.shape[1]  # total predictions made\n",
    "    print('m of: ' + str(m))\n",
    "    \n",
    "    lhs_with_dot = np.dot(y, np.log(y_hat).T) # returns 1x1 array\n",
    "    rhs_with_dot = np.dot((1 - y), np.log(1 - y_hat).T)\n",
    "    \n",
    "    lhs = np.multiply(test_labs, np.log(dummy_prediction)) # element-wise returns 1 * m array\n",
    "    rhs = np.multiply((1 - y), np.log(1 - y_hat))\n",
    "    \n",
    "    \"\"\"\n",
    "    print(rhs_with_dot.shape)    \n",
    "    print(rhs.shape)    \n",
    "    print(np.sum(rhs_with_dot))\n",
    "    print(np.sum(rhs))\n",
    "    \"\"\"\n",
    "\n",
    "    total_loss = np.sum(lhs + rhs)\n",
    "    return -total_loss / m\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-acoustic",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "permanent-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(input_data, y, lr, iterations):\n",
    "    \"\"\"Gradient descent to minimise cost function\n",
    "    \n",
    "    No complicated use of chain rule yet as no NN\n",
    "    lr = Learning rate; magnitude of updates on each iteration\n",
    "    \n",
    "    \n",
    "    dz = (y_hat - y) / m  # differential of z (linear regression stage)\n",
    "                        m = sample size\n",
    "                        \n",
    "    dw1 = x1 * dz  # x1 = \n",
    "    dw2 = x2 * dz\n",
    "    .. etc\n",
    "    db = b * dz\n",
    "    \n",
    "    For more on where the above formulae come from:\n",
    "    https://www.coursera.org/learn/neural-networks-deep-learning/lecture/5sdh6/logistic-regression-gradient-descent\n",
    "    \n",
    "    \n",
    "    w = w - lr*dw, where dw = slope of parameters (ie, derivative of cost func with respect to w)\n",
    "    b = b - lr*db    # same idea, for intercept value\n",
    "    \n",
    "    \n",
    "   \"\"\"\n",
    "    w_input_params_len = input_data.shape[0]\n",
    "    w = np.random.uniform(size=(w_input_params_len, 1)) # make of size m by 1\n",
    "    b = np.random.uniform(size=(1, 1))\n",
    "    \n",
    "    assert(w.shape == (w_input_params_len, 1))  # raises error if not true\n",
    "    \n",
    "    print(input_data.shape)\n",
    "    print(w.T.shape)\n",
    "    \n",
    "    \n",
    "    # starting gradient descent loop\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        # compute linear transformation of inputs\n",
    "        linear_result = np.dot(w.T, input_data) + b  # returns m * 1 array\n",
    "        linear_result = linear_result.astype(np.float)  # ensure is float; needed for np.exp() to work\n",
    "        #print('linear_result shape: ' + str(linear_result.shape))\n",
    "\n",
    "        \"\"\"\n",
    "        print(linear_result.shape)\n",
    "        print(linear_result.ndim)\n",
    "        print(linear_result.dtype)\n",
    "        \"\"\"\n",
    "\n",
    "        y_hat = 1 / (1 + np.exp(-linear_result * 1.0))  # apply logistic func for each input value\n",
    "        #print('y_hat shape: ' + str(y_hat.shape))\n",
    "\n",
    "        # get weight derivatives\n",
    "        dz = y_hat - y           # think 'z' refers to the linear_result stage\n",
    "                                # dz has a value for each input datum\n",
    "\n",
    "        db = np.sum(dz) / m\n",
    "\n",
    "\n",
    "        dw = np.dot(1/m, dz.T)  # should dw be 4,1 not 120,1 dimensions, or a single value\n",
    "        #print(dw.shape)\n",
    "        dw = np.mean(dw)   # fudging it until understand further: making it a single number for now\n",
    "                        # which means all weights will update by an equal amount in the gradient descent\n",
    "                        # which doesnt seem optimal\n",
    "\n",
    "\n",
    "        # make new weights\n",
    "        w = w - lr * dw\n",
    "        b = b - lr * db\n",
    "        \n",
    "    return w, b\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "postal-creature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 120)\n",
      "(1, 4)\n",
      "[[0.06706159]\n",
      " [0.24803914]\n",
      " [0.43868117]\n",
      " [0.4569271 ]]\n",
      "[[-2.441317]]\n"
     ]
    }
   ],
   "source": [
    "## getting weights\n",
    "w, b = gradient_descent(train_input, train_labs, 0.1, 10000)\n",
    "print(w)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "square-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying to test set\n",
    "linear_result = np.dot(w.T, test_input) + b\n",
    "linear_result = linear_result.astype(np.float)\n",
    "predictions = 1 / (1 + np.exp(-linear_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "included-emission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x12da5d880>"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluating accuracy on test set: doesnt seem to differentiate between classes too well\n",
    "plt.scatter(predictions, test_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "later-assistant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommends always making arrays 2d to avoid confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "spiritual-fellowship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m of: 30\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0522477564544752"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test cost function\n",
    "dummy_prediction = np.random.uniform(size=30)\n",
    "dummy_prediction = dummy_prediction.reshape(1, dummy_prediction.shape[0])\n",
    "\n",
    "cost = cost_function(test_labs, dummy_prediction)\n",
    "cost\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
