{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "intended-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## making a basic shallow NN in numpy, based on week 3 of Neural Networks and Deep Learning\n",
    "\n",
    "## Aiming to predict whether a plant is species setosa or not\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeplearning.ai notation: [1] = activations out of layer 1, [2] = activitions from layer 2, etc\n",
    "# instead of backprop from A (sig transformed) to Z (linear transformed) and that's it, that\n",
    "# is done several times a la A, Z, A, Z, etc for backprop, depending on the number of layers you have\n",
    "# a(i) refers to node i in that layer\n",
    "\n",
    "# '2 layer' NN has one hidden layer (plus input and output layers)\n",
    "\n",
    "# if 4 weights and 5 neurons in a hidden layer, w in that layer will have dims (5, 4) and b will have dims (5, 1)\n",
    "# and if so, output layer after this was w dims (1, 5) and b dims (1, 1)\n",
    "\n",
    "### hidden layer\n",
    "# multiply (5,4) hidden layer weights by (4, 1)  input features array (assuming 4 is number of input features)\n",
    "# (4,1) array might become (4, m) where m is training sample size\n",
    "# assuming input is (4, 1), gets a result of (5, 1) which goes through sigmoid to get A=(5,1) array\n",
    "\n",
    "### output layer\n",
    "# multiply weights layer (1, 5) by input (5,1), plus b for linear trans to get (1,1)\n",
    "# then sigmoid transform to get (1,1) output\n",
    "\n",
    "\n",
    "\n",
    "#####\n",
    "# x :dims = n * m (input of n features and m data)\n",
    "# Z (output of linear regession), dims = lc * m\n",
    "# A (output of sigmoid), dims = lc * m (lc = layer count)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "## steps:\n",
    "\n",
    "# forward pass through layers\n",
    "# get derivatives for all weights\n",
    "# calculate new weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "female-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation functions:\n",
    "# denote generic activation function as g(x)\n",
    "\n",
    "# sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(sigmoid_output):\n",
    "    return sigmoid_output * (1- sigmoid_output)   # * = element-wise mult; @ = matrix mult\n",
    "\n",
    "\n",
    "\n",
    "# tanh. A shifted version of sigmoid (returns range -1 to 1 instead of 0 to 1)\n",
    "    # the mean of tanh is closer to 0 than sigmoid, which can make the outputs easier for later layers to use\n",
    "    # than sigmoid, where the mean is closer to 0.5\n",
    "    # says apart from the output layer, tanh() is better than sigmoid\n",
    "def tanh(x):\n",
    "    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))\n",
    "\n",
    "def tanh_derivative(tanh_output):\n",
    "    return 1 - np.power(tanh_output, 2)\n",
    "\n",
    "    \n",
    "\n",
    "# ReLu: better for larger values as the gradient doesn't become close to 0, which it does for tanh and sigmoid\n",
    "# ReLu trains faster because of this \n",
    "def relu(x):\n",
    "    return np.where(x >= 0, x, 0)\n",
    "\n",
    "def relu_derivative(x):       \n",
    "    \"\"\"note x is the original input to relu, not the output from relu, unlike other\n",
    "    derivatives of activation functions\"\"\"\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "# Leaky ReLu: max(0.01*x, x)  # so not exactly zero for negative numbers\n",
    "def leaky_relu(x):\n",
    "    return np.where(x >= 0, x, x*0.01)\n",
    "\n",
    "def leaky_relu_derivative(x):\n",
    "    \"\"\"As with ReLu derivative, x is the original input to leaky_relu(), not it's input\"\"\"\n",
    "    return np.where(x >= 0, 1, 0.01)\n",
    "\n",
    "\n",
    "\n",
    "### rules of thumb\n",
    "# if classification is binary then sigmoid is a good output layer activation\n",
    "# for all other units ReLu is the most popular default, so a decent default choice\n",
    "\n",
    "\n",
    "\n",
    "# source of activation function derivative functions:\n",
    "# https://www.coursera.org/learn/neural-networks-deep-learning/lecture/qcG1j/derivatives-of-activation-functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "wooden-mustang",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'A_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e0fb0ce82c57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# using _ as stand-in for [] notation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mdZ_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mdW_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdZ_2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mA_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mdB_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# sums horizontally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'A_2' is not defined"
     ]
    }
   ],
   "source": [
    "#### Gradient descent\n",
    "## Params: W[1], b[1], W[2], b[2]   (represent weights and intersect value for each layer)\n",
    "## n[0] (input features count), n[1] (hidden unit count), n[2] (output units)\n",
    "\n",
    "# W[1] dims: n[1] * n[0]\n",
    "# B[1] dims: n[1] * 1\n",
    "# W[2] dims: n[2] * n[1]\n",
    "# B[2] dims: n[2] * 1\n",
    "\n",
    "# X = input data\n",
    "# Z[1] = hidden layer linear results       dims: n[1] * 1\n",
    "# A[1] = hidden layer activated results\n",
    "# Z[1] = output layer linear results         dims: n[2] * 1\n",
    "# A[1] = output layer activated results\n",
    "\n",
    "# Y dims: m * 1  (where m is number of data in)\n",
    "# A[2] dims: m * 1  (the prediction: result of output activation)\n",
    "\n",
    "# can use same cost function as logistic regression (think this is because output layer using sigmoid activation)\n",
    "# J = notation for cost\n",
    "\n",
    "# dW[1] = dJ/dW[1] \n",
    "# dB[1] = dJ/dB[1] \n",
    "# ... and so on for all layers\n",
    "\n",
    "\n",
    "# using _ as stand-in for [] notation\n",
    "\"\"\"\n",
    "dZ_2 = A_2 - Y                           # dims: n[2] * m\n",
    "dW_2 = (1/m) * (dZ_2 @ A_1.T)\n",
    "dB_2 = (1/m) * np.sum(dZ_2, axis=1, keepdims=True)  # sums horizontally\n",
    "\n",
    "dZ_1 = (W_2.T @ dZ_2) * relu_derivative(Z_1)   # dims: n[1] * m (where m is number of data inputs)\n",
    "        # W_2.T dims = n[1] * n[2]\n",
    "        # dZ_2 dims = n[2] * m\n",
    "        # relu_derivative(Z_1) dims = n[1] * m\n",
    "        ### so get n[1] * m output from matrix mult, then do element wise mult with relu_derivative(Z_1)\n",
    "\n",
    "dW_1 = (1/m) * (dZ_1 @ X.T)\n",
    "dB_1 = (1/m) * np.sum(dZ_1, axis=1, keepdims=True) \n",
    "\"\"\"\n",
    "## confident all of the above use @ and * in the right places\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# based on:\n",
    "# https://www.coursera.org/learn/neural-networks-deep-learning/lecture/6dDj7/backpropagation-intuition-optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "amazing-vault",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### when initialising weights\n",
    "#### this is just an example: the input dimensions are wrong here\n",
    "\n",
    "W_weights = np.random.randn(2, 2) / 100   # set small random weights for W    \n",
    "B_weights = np.zeros((2, 1))           # so long as W has random weights, says zeros is a fine init val for B\n",
    "\n",
    "# initial weights should be very small, otherwise values that go thru tanh or sigmoid will \n",
    "# have tiny weights and learn very slowly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-front",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "social-trash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 5\n",
      "(150, 5)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data = pd.read_csv('https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv')\n",
    "data = data.to_numpy()\n",
    "\n",
    "\n",
    "# unique, counts = np.unique(data[:, 4], return_counts=True)\n",
    "setosa_idx = data[:, 4] == 'setosa'  # simplify species column to binary\n",
    "data[:, 4] = 1   # not setosa\n",
    "data[setosa_idx, 4] = 0   # setosa\n",
    "\n",
    "\n",
    "idx = np.random.rand(*data.shape).argsort(axis=0) # randomising order for test/train split\n",
    "data = np.take_along_axis(data,idx,axis=0)\n",
    "\n",
    "\n",
    "print(*data.shape)  # return values outside tuple\n",
    "print(data.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "temporal-organizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 120)\n",
      "(1, 120)\n"
     ]
    }
   ],
   "source": [
    "train_input = data[:120, :4].T   # transpose to row for each feature and column for each value: faster calcs\n",
    "train_labs = data[:120, 4:].T\n",
    "\n",
    "test_input = data[120:, :4].T\n",
    "test_labs = data[120:, 4:].T\n",
    "\n",
    "print(train_input.shape)\n",
    "print(train_labs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "valuable-aircraft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5 0.5 0 0.5 0.5 0.5 0 0.5 0 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0 0 0.5\n",
      "  0.5 0.5 0 0.5 0.5 0 0 0.5 0.5 0]]\n",
      "[[1 1 0 1 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 1 1 0 0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# test relu\n",
    "x = relu(test_labs - 0.5)\n",
    "print(x)\n",
    "print(relu_derivative(test_labs - 0.5))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "electric-hawaiian",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labs_values: [0 1]\n",
      "labs_counts: [41 79]\n"
     ]
    }
   ],
   "source": [
    "# view test labs frequency\n",
    "labs_values, labs_counts = np.unique(train_labs, return_counts = True)\n",
    "print('labs_values: ' + str(labs_values))\n",
    "print('labs_counts: ' + str(labs_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "virtual-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(y, y_hat):   \n",
    "    \"\"\"Logistic regression cost function\n",
    "    y = actual\n",
    "    y_hat = predicted\n",
    "    \"\"\"\n",
    "    m = y.shape[1]  # total predictions made\n",
    "    lhs = np.dot(y, np.log(y_hat).T) # returns 1x1 array\n",
    "    rhs = np.dot((1 - y), np.log(1 - y_hat).T)\n",
    "    total_loss = np.sum(lhs + rhs)\n",
    "    return -total_loss / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "permanent-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, n_layer, lr, iterations):\n",
    "    \"\"\"\n",
    "    X = input table (n * m)\n",
    "    Y = labels    (1 * m)\n",
    "    lr = Learning rate: how much weights change on each iteration\n",
    "   \"\"\"\n",
    "    \n",
    "    loss_store = np.zeros(iterations)  # to store loss on each iteration\n",
    "    \n",
    "    \n",
    "    # set dimension values used \n",
    "    n_0 = X.shape[0]   # total features \n",
    "    n_1 = n_layer         # number of neurons in hidden layer\n",
    "    n_2 = 1         # output units: think it's 1 as only one layer. Could be 2 if linked to binary outcome somehow\n",
    "    \n",
    "\n",
    "    m = X.shape[1]   # size of input data\n",
    "    \n",
    "    # initialise weights             \n",
    "    W_1 = np.random.uniform(size=(n_1, n_0)) / 100\n",
    "    B_1 = np.random.uniform(size=(n_1, 1))  / 100\n",
    "    W_2 = np.random.uniform(size=(n_2, n_1))  / 100\n",
    "    B_2 = np.random.uniform(size=(n_2, 1))  / 100\n",
    "    \n",
    "    # W[1] dims: n[1] * n[0]\n",
    "    # B[1] dims: n[1] * 1\n",
    "    # W[2] dims: n[2] * n[1]\n",
    "    # B[2] dims: n[2] * 1\n",
    "    \n",
    "    #### starting gradient descent loop\n",
    "    for i in range(iterations):\n",
    "\n",
    "\n",
    "        # hidden layer: compute linear transformation of inputs\n",
    "        Z_1 = np.dot(W_1, X) + B_1  # returns 5 * m array  (as there are 5 neurons)\n",
    "        #print(Z_1.shape)\n",
    "\n",
    "        # hidden layer: activation func\n",
    "        A_1 = relu(Z_1)\n",
    "        #print(A_1.shape)\n",
    "\n",
    "\n",
    "        # output layer: linear transformation\n",
    "        Z_2 = np.dot(W_2, A_1) + B_2\n",
    "        Z_2 = Z_2.astype(np.float) # ensure is float; needed for sigmoid() to work\n",
    "        #print(Z_2.shape)\n",
    "\n",
    "        # output layer: activation func\n",
    "        A_2 = sigmoid(Z_2)\n",
    "        #print(A_2.shape)\n",
    "\n",
    "\n",
    "        # store loss of predictions vs actual\n",
    "        #print('Y: '+ str(Y))\n",
    "        #print('A_2: ' + str(A_2))\n",
    "        loss_store[i] = cost_function(Y, A_2)\n",
    "        #print('loss store: ' + str(loss_store[i]))\n",
    "\n",
    "\n",
    "        # output layer: get derivatives\n",
    "        dZ_2 = A_2 - Y                           # dims: n[2] * m\n",
    "        #print(dZ_2.shape)\n",
    "\n",
    "        dW_2 = (1/m) * (dZ_2 @ A_1.T)\n",
    "        #print(dW_2.shape)\n",
    "\n",
    "        dB_2 = (1/m) * np.sum(dZ_2, axis=1, keepdims=True)  # sums horizontally\n",
    "        #print(dB_2.shape)\n",
    "\n",
    "\n",
    "        # hidden layer: get derivatives \n",
    "        dZ_1 = (W_2.T @ dZ_2) * relu_derivative(Z_1)   # dims: n[1] * m (where m is number of data inputs)\n",
    "                # W_2.T dims = n[1] * n[2]\n",
    "                # dZ_2 dims = n[2] * m\n",
    "                # relu_derivative(Z_1) dims = n[1] * m\n",
    "                ### so get n[1] * m output from matrix mult, then do element wise mult with relu_derivative(Z_1)\n",
    "        #print(dZ_1.shape)\n",
    "\n",
    "        dW_1 = (1/m) * (dZ_1 @ X.T)\n",
    "        #print(dW_1.shape)\n",
    "\n",
    "        dB_1 = (1/m) * np.sum(dZ_1, axis=1, keepdims=True)\n",
    "        #print(dB_1.shape)\n",
    "\n",
    "\n",
    "\n",
    "        # update weights\n",
    "        W_1 = W_1 - lr * dW_1\n",
    "        B_1 = B_1 - lr * dB_1\n",
    "        W_2 = W_2 - lr * dW_2\n",
    "        B_2 = B_2 - lr * dB_2\n",
    "\n",
    "\n",
    "\n",
    "    return W_1, B_1, W_2, B_2, loss_store\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "postal-creature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.019038707576446122 0.01992479050719476 0.10219754990603036\n",
      "  0.06777390608035769]\n",
      " [-0.02055992195520091 0.02025746657913673 0.08545514077059663\n",
      "  0.04889520779632483]\n",
      " [-0.031344800281566186 0.032325167830948735 0.12898443358425113\n",
      "  0.07464021827030544]\n",
      " [-0.015883762226229264 0.014837312627304594 0.06139661354707217\n",
      "  0.04420091045104993]\n",
      " [-0.027429954223503535 0.03159995446094954 0.12361476926868295\n",
      "  0.07922880284326346]\n",
      " [-0.026328401392267095 0.027965126448621924 0.12785356018649083\n",
      "  0.07868662623945728]\n",
      " [-0.026257201377177377 0.022650263160464864 0.11213622482276533\n",
      "  0.068699792532909]\n",
      " [-0.034767059318643705 0.027884693252226804 0.14816600039666936\n",
      "  0.0928369109126145]\n",
      " [-0.015876127804617845 0.021287186253585406 0.08336107002832255\n",
      "  0.0501188970612519]\n",
      " [-0.030748045590819922 0.02945614428771853 0.12870471199872616\n",
      "  0.08221666008403343]]\n",
      "[[-0.010978889477657891]\n",
      " [-0.007917462489131942]\n",
      " [-0.008195065227835542]\n",
      " [-0.004303600418969409]\n",
      " [-0.007527381289411512]\n",
      " [-0.007484254281593231]\n",
      " [-0.011130028278584142]\n",
      " [-0.010952268896451768]\n",
      " [-0.005092895681944953]\n",
      " [-0.007323240772557911]]\n",
      "[[0.1257560323153866 0.10291337887025274 0.1557584822992351\n",
      "  0.0780933333702558 0.15208913407150204 0.1550026579830318\n",
      "  0.1359155934732497 0.18030829365352957 0.10029044704648513\n",
      "  0.1584141761289153]]\n",
      "[[-0.010978889477657891]\n",
      " [-0.007917462489131942]\n",
      " [-0.008195065227835542]\n",
      " [-0.004303600418969409]\n",
      " [-0.007527381289411512]\n",
      " [-0.007484254281593231]\n",
      " [-0.011130028278584142]\n",
      " [-0.010952268896451768]\n",
      " [-0.005092895681944953]\n",
      " [-0.007323240772557911]]\n"
     ]
    }
   ],
   "source": [
    "## training model and viewing weights\n",
    "W_1, B_1, W_2, B_2, loss_store = gradient_descent(train_input, train_labs, 10, 0.01, 5000)\n",
    "print(W_1)\n",
    "print(B_1)\n",
    "print(W_2)\n",
    "print(B_1)\n",
    "\n",
    "\n",
    "## sigmoid encounters overflow on training. Ways around this:\n",
    "# clipping gradients outside acceptable boundaries\n",
    "\n",
    "\n",
    "\n",
    "## Likely this is messing up training, as atm it predicts 0's for all, which isn't overfitting, as \n",
    "## predicting all 1's would get a better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "continuing-election",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x135852c10>]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# viewing trend in loss\n",
    "plt.plot(loss_store)\n",
    "#loss_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "square-sentence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying to test set\n",
    "Z_1 = np.dot(W_1, test_input) + B_1\n",
    "A_1 = relu(Z_1)\n",
    "Z_2 = np.dot(W_2, A_1) + B_2\n",
    "Z_2 = Z_2.astype(np.float) \n",
    "predictions = sigmoid(Z_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "included-emission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x13589ffd0>"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# evaluating accuracy on test set: nothing to write home about\n",
    "plt.scatter(predictions, test_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "later-assistant",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.65463326, 0.73100814, 0.6942272 , 0.71791519, 0.70271266,\n",
       "        0.72756275, 0.59807019, 0.61354685, 0.59615428, 0.68209287,\n",
       "        0.73027719, 0.55093401, 0.72565872, 0.73248185, 0.69637838,\n",
       "        0.5818068 , 0.65023379, 0.64444493, 0.75649049, 0.57319804,\n",
       "        0.73521948, 0.76526855, 0.65202435, 0.6899524 , 0.5539079 ,\n",
       "        0.59252686, 0.72166662, 0.59110417, 0.67763687, 0.7364684 ]])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-fellowship",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
