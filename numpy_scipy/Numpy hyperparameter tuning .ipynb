{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "intended-practitioner",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Notes on Hyperparameter Tuning, Batch Normalization and Programming Frameworks \n",
    "# (week 3 of course 2 in deep learning specialisation)\n",
    "\n",
    "\n",
    "# to improve: use softmax to predict plant species, batch norm, hyperparam search\n",
    "# at present: have softmax implemented BUT the weights all come out as NaN: think the dimensions of \n",
    "# some values aren't squaring up\n",
    "\n",
    "\n",
    "\n",
    "## Aiming to predict which species a plant is\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "harmful-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "### hyperparams, his take on what are most important\n",
    "# learning rate is most important hyperparam to get right\n",
    "# 2nd in importance: no. of nodes in layers, beta, mini-batch size\n",
    "# 3rd: learning rate decay, number of layers\n",
    "# less important: beta1, beta2, epsilon (all the ones used in Adam optimisation)\n",
    "\n",
    "# though IRL it can be hard to know in advance which hyperparams will most impact your particular NN\n",
    "\n",
    "# when selecting samples for tuning: better to have some randomness in all dimensions of possible hyperparams,\n",
    "# not a grid. As if one dimension (ie one hyperparam) turns out to be way more important than the others, you \n",
    "# will have a good variety of values tried\n",
    "\n",
    "\n",
    "# 'Coarse to fine' approach: sampling broadly in possible hyperparam space, then zooming and sampling from smaller\n",
    "# area which is returning most promising values\n",
    "\n",
    "\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "meaning-florida",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Batch normalisation\n",
    "# normalising the inputs for each layer (not just the input array X)\n",
    "# \n",
    "# \n",
    "# batch norm is applied after linear transformation but before the activation function\n",
    "# \n",
    "\n",
    "# as batch norm changes the mean to be zero, the B term in WX + B becomes irrelevant, so if\n",
    "# using batch norm you can stop using B, and just search for weights to multiply the input array by\n",
    "# In practice the beta input to tilda_norm_layer_output ends up doing a very similar job, as it \n",
    "# sets the mean: so this is trained in gradience descent instead (as is gamma)\n",
    "\n",
    "\n",
    "def norm_layer_output(z_output):\n",
    "    epsilon = 10**-8   # tiny number to ensure denominator is never exactly zero\n",
    "    \n",
    "    mu = np.mean(z_output)\n",
    "    sigma_squared = np.var(z_output)\n",
    "    return (z_output - mu) / np.sqrt(sigma_squared + epsilon)\n",
    "\n",
    "\n",
    "# the above sets variance to one and mean to zero for hidden layer output, and in \n",
    "# some cases that won't be what you want\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tilda_norm_layer_output(normed_z_output, gamma, beta):\n",
    "    \"\"\"Takes result of norm_layer_output() as an input\n",
    "    gamma rescales the variance of the data\n",
    "    beta changes the mean of the data\n",
    "    \n",
    "    gamma dims = n[l] * 1   # eg: if a layer has 5 nodes, this will have dimensions 5*1\n",
    "    beta dims = n[l] * 1\n",
    "    \n",
    "    optimal gamma and beta values can be learned as part of gradient descent process: do this\n",
    "    by treating them as another few weights to learn within that layer\n",
    "    \n",
    "    note beta in this context is quite separate to beta in other contexts, eg: Adam optimising\n",
    "    \"\"\"\n",
    "    return (gamma * normed_z_output) + beta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# for the formula:\n",
    "# https://www.coursera.org/learn/deep-neural-network/lecture/4ptp2/normalizing-activations-in-a-network\n",
    "\n",
    "\n",
    "# batch norm reduces the amount later layers have to adapt to changes (of weights) in earlier layers\n",
    "# thus speeds training\n",
    "# \n",
    "# similar to dropout, mini-batch and batch norming adds a little noise, as training on only a sample of \n",
    "# the full training set\n",
    "# I think batch norming is in the same bucket as mini-batch on this because it adds extra weights to \n",
    "# train, which means more influence/noise from a given training iteration\n",
    "\n",
    "# Says this regularisation effect is fairly small, and more of a side effect than something you can count on\n",
    "\n",
    "# Noise forces later layers not to overfit to earlier layer output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "declared-seminar",
   "metadata": {},
   "outputs": [],
   "source": [
    "### applying model to batch norm:\n",
    "# might only want to put one data thru it, so can't normalise by variance of sample\n",
    "# so estimate the best mu and variance values using training set prior to applying model\n",
    "\n",
    "# this is done with exponential weighted average across the mu and variance values\n",
    "# for mini-batches on the final epoch (or I think it's the final epoch: not much gains to using older epochs)\n",
    "\n",
    "# \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "identified-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiple classes: softmax regression\n",
    "\n",
    "# output layer has count of nodes equal to number of classes (classification options)\n",
    "\n",
    "# each node in output layer outputs probability that the data is of the class that node represents\n",
    "\n",
    "# the total probabilities should sum to one (denominator in function below does this)\n",
    "\n",
    "def softmax_activation(input_array):\n",
    "    \"\"\"data still needs to go through linear transform before applying this activation func\"\"\"\n",
    "    return np.exp(input_array) / np.sum(np.exp(input_array))\n",
    "\n",
    "\n",
    "# if no. of classes=2, softmax is same as logistic activation\n",
    "# softmax is a generalisation of logistic activation to N classes\n",
    "\n",
    "\n",
    "\n",
    "# for softmax:\n",
    "# https://www.coursera.org/learn/deep-neural-network/lecture/HRy7y/softmax-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "driving-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def softmax_single_data_point_loss(predicted_single_data, actual_single_data):\n",
    "    \"\"\"Both inputs are n*1 arrays, where n is number of classes\n",
    "    \n",
    "    Not used as it's easier to implement this inside the softmax_cost function below\n",
    "    \"\"\"\n",
    "    return -np.sum(actual * np.log(predicted)) \n",
    "\n",
    "\n",
    "def softmax_cost(predicted, actual):\n",
    "    \"\"\"Aggregates losses across full batch\n",
    "    \n",
    "    The course video shows two separate functions: one for each data and one summing all results, \n",
    "    however it seems easier to do them both in one line\n",
    "    \n",
    "    \"actual\" should be 1s and 0s\n",
    "    \"\"\"\n",
    "    m = predicted.shape[0]\n",
    "    \n",
    "    all_data_loss = -np.sum(actual * np.log(predicted)) / m  # sum across columns\n",
    "    \n",
    "    return all_data_loss\n",
    "\n",
    "\n",
    "def derivative_cost_wrt_softmax(predicted, actual):\n",
    "    \"\"\"Same as for logistic function\"\"\"\n",
    "    return predicted - actual\n",
    "\n",
    "\n",
    "\n",
    "# for softmax loss func:\n",
    "# https://www.coursera.org/learn/deep-neural-network/lecture/LCsCH/training-a-softmax-classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "floating-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for regression NN (my ideas)\n",
    "# use a regression cost function eg sum of squared errors\n",
    "# otherwise keep everything the same... it might work\n",
    "# output layer could have multiple dimensions (eg: if you're trying to predict 2+ numbers for each data), the\n",
    "        # sum of which vs actual values could feed the cost function\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "numerous-facility",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial value of w: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>\n",
      "new value of w: <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.09999997>\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(0, dtype=tf.float32)\n",
    "optimiser = tf.keras.optimizers.Adam(0.1)\n",
    "\n",
    "def train_step():\n",
    "    \"\"\"single step towards minimising cost function (quadratic in this case)\"\"\"\n",
    "    with tf.GradientTape() as tape:   # idea is it's a \"tape\" you play forward, and then rewind the tape as backprop\n",
    "        cost = w**2 + 10*w + 25\n",
    "    trainable_variables = [w]   # list of variable names - only one in this case\n",
    "    grads = tape.gradient(cost, trainable_variables)\n",
    "    optimiser.apply_gradients(zip(grads, trainable_variables))  #zip() pairs corresponding elements in inputs\n",
    "\n",
    "\n",
    "print('initial value of w: ' + str(w))     \n",
    "train_step()\n",
    "print('new value of w: ' + str(w))   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "valued-equipment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-5.000001>\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    train_step()\n",
    "    \n",
    "print(w)   # converges on value that minimises cost func: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "raising-patrol",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only have to write forward propagation stage in tensorflow: it deals with the backprop for you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-coalition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so in tensorflow you just define the cost function, nn structures, optimiser, and maybe a few\n",
    "# other things, and you're golden\n",
    "\n",
    "\n",
    "# tensorflow automatically calculates the derivative of your cost function and all subsequent derivatives\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-sequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorflow is available in c++, but it's pretty damn optimised in python already\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-festival",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-location",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "editorial-humanitarian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-reach",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-profit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approximate-affairs",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "female-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Activation functions:\n",
    "# denote generic activation function as g(x)\n",
    "\n",
    "# sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(sigmoid_output):\n",
    "    return sigmoid_output * (1- sigmoid_output)   # * = element-wise mult; @ = matrix mult\n",
    "\n",
    "\n",
    "# ReLu: better for larger values as the gradient doesn't become close to 0, which it does for tanh and sigmoid\n",
    "# ReLu trains faster because of this \n",
    "def relu(x):\n",
    "    return np.where(x >= 0, x, 0)\n",
    "\n",
    "def relu_derivative(x):       \n",
    "    \"\"\"note x is the original input to relu, not the output from relu, unlike other\n",
    "    derivatives of activation functions\"\"\"\n",
    "    return np.where(x >= 0, 1, 0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wooden-mustang",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "amazing-vault",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-front",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "social-trash",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv('https://gist.githubusercontent.com/curran/a08a1080b88344b0c8a7/raw/0e7a9b0a5d22642a06d3d5b9bcbad9890c8ee534/iris.csv')\n",
    "data = data.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "# randomising order for test/train split\n",
    "idx = np.random.rand(*data.shape).argsort(axis=0) \n",
    "data = np.take_along_axis(data,idx,axis=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# dummy code labels (3 by 150 dimensions - for logistic activation it was 1d, now 2d)\n",
    "setosa_idx = data[:, 4] == 'setosa' \n",
    "virginica_idx = data[:, 4] == 'virginica' \n",
    "versicolor_idx = data[:, 4] == 'versicolor' \n",
    "labels_dummy = np.concatenate((setosa_idx, virginica_idx, versicolor_idx)).reshape(3, 150).astype(np.float32)\n",
    "\n",
    "\n",
    "# normalise first 4 columns\n",
    "for i in range(4): \n",
    "    norms_each_column = np.linalg.norm(data[:, i], axis = 0)\n",
    "    data[:, i] = data[:, i] / norms_each_column\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "# split into train and test\n",
    "train_input = data[:120, :4].T   # transpose to row for each feature and column for each value: faster calcs\n",
    "test_input = data[120:, :4].T\n",
    "\n",
    "train_labs = labels_dummy[:, :120]\n",
    "test_labs = labels_dummy[:, 120:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-organizer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-aircraft",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "electric-hawaiian",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward_relu_layer(X, W, B):\n",
    "    \"\"\"\n",
    "    X = input data \n",
    "    W = input weights\n",
    "    B = input bias\n",
    "    \n",
    "    Exports Z as it's needed for backpropagation\n",
    "    \"\"\"\n",
    "    Z = np.dot(W, X) + B\n",
    "    return relu(Z), Z  # returns 2 n * m arrays  (n = number of neurons)\n",
    "\n",
    "\n",
    "def backpropagate_relu_layer(X, Z, dZ_nextLayer, W_nextLayer):\n",
    "    \"\"\"\n",
    "    X = input at start of the layer (from input layer or previous hidden layer)\n",
    "    Z = values after X been through linear calculation (yx + b)\n",
    "    dZ_nextLayer = dZ from layer next-closest to output layer\n",
    "    W_nextLayer = W from layer next-closest to output layer\n",
    "    \"\"\"\n",
    "    m = X.shape[1]                                 # total input data in training set\n",
    "    dZ_thisLayer = (W_nextLayer.T @ dZ_nextLayer) * relu_derivative(Z)  \n",
    "    dW_this_layer = (1/m) * (dZ_thisLayer @ X.T)\n",
    "    dB_this_layer = (1/m) * np.sum(dZ_thisLayer, axis=1, keepdims=True)\n",
    "    return dZ_thisLayer, dW_this_layer, dB_this_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "virtual-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(y, y_hat):   \n",
    "    \"\"\"Logistic regression cost function\n",
    "    y = actual\n",
    "    y_hat = predicted\n",
    "    \"\"\"\n",
    "    m = y.shape[1]  # total predictions made\n",
    "    lhs = np.dot(y, np.log(y_hat).T) # returns 1x1 array\n",
    "    rhs = np.dot((1 - y), np.log(1 - y_hat).T)\n",
    "    total_loss = np.sum(lhs + rhs)\n",
    "    return -total_loss / m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "upset-aging",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_adam_parameters(v_dw, v_db, s_dw, s_db, dW, dB, beta1, beta2):\n",
    "    v_dw_new = beta1 * v_dw + (1-beta1) * dW    # same formula as momentum\n",
    "    v_db_new = beta1 * v_db + (1-beta1) * dB\n",
    "    s_dw_new = beta2 * s_dw + (1-beta2) * np.power(dW, 2)   # same formula as RMSprop\n",
    "    s_db_new = beta2 * s_db + (1-beta2) * np.power(dB, 2)\n",
    "        \n",
    "    v_dw_new = v_dw_new.astype(float)\n",
    "    v_db_new = v_db_new.astype(float)\n",
    "    s_dw_new = s_dw_new.astype(float)\n",
    "    s_db_new = s_db_new.astype(float)\n",
    "    return v_dw_new, v_db_new, s_dw_new, s_db_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toxic-sphere",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "permanent-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, n_layer, lr_init, iterations, decay_rate, beta1=0.9, beta2=0.999):\n",
    "    \"\"\"\n",
    "    Trains parameters for NN with 3 hidden layers, and output layer with sigmoid activation\n",
    "    \n",
    "    X = input table (n * m)\n",
    "    Y = labels    (1 * m)\n",
    "    lr = Learning rate: how much weights change on each iteration\n",
    "   \"\"\"\n",
    "    \n",
    "    loss_store = np.zeros(iterations)  # to store loss on each iteration\n",
    "    \n",
    "    epsilon = 10**-8  # used in Adam optimiser\n",
    "    \n",
    "    \n",
    "    # set dimension values used \n",
    "    n_0 = X.shape[0]   # total features \n",
    "    n_1 = n_layer         # number of neurons in hidden layer\n",
    "    n_2 = 3         # output units: 1 if sigmoid output activation, more if softmax\n",
    "    \n",
    "\n",
    "    m = X.shape[1]   # size of input data\n",
    "    \n",
    "    # initialise weights             \n",
    "    W_1_1 = np.random.randn(n_1, n_0) * np.sqrt(2 / n_0)  # notice weights in first layer have different dims\n",
    "    W_1_2 = np.random.randn(n_1, n_1) * np.sqrt(2 / n_1) \n",
    "    W_1_3 = np.random.randn(n_1, n_1) * np.sqrt(2 / n_1) \n",
    "    B_1_1 = np.random.randn(n_1, 1) * np.sqrt(2 / n_1) \n",
    "    B_1_2 = np.random.randn(n_1, 1) * np.sqrt(2 / n_1) \n",
    "    B_1_3 = np.random.randn(n_1, 1) * np.sqrt(2 / n_1) \n",
    "    \n",
    "    W_2 = np.random.randn(n_2, n_1) * np.sqrt(2 / n_1) \n",
    "    B_2 = np.random.randn(n_2, 1) * np.sqrt(2 / n_1) \n",
    "\n",
    "    \n",
    "    # group training data for mini-batch\n",
    "    mini_batch_groups = np.split(X, 6, axis = 1)\n",
    "    mini_batch_group_labels = np.split(Y, 6, axis = 1)\n",
    "    \n",
    "    \n",
    "    # initialise values for Adam optimiser\n",
    "    v_dw_1_1 = 0.0\n",
    "    s_dw_1_1 = 0.0\n",
    "    v_db_1_1 = 0.0\n",
    "    s_db_1_1 = 0.0\n",
    "    \n",
    "    v_dw_1_2 = 0.0\n",
    "    s_dw_1_2 = 0.0\n",
    "    v_db_1_2 = 0.0\n",
    "    s_db_1_2 = 0.0\n",
    "    \n",
    "    v_dw_1_3 = 0.0\n",
    "    s_dw_1_3 = 0.0\n",
    "    v_db_1_3 = 0.0\n",
    "    s_db_1_3 = 0.0\n",
    "    \n",
    "    v_dw_2 = 0.0\n",
    "    s_dw_2 = 0.0\n",
    "    v_db_2 = 0.0\n",
    "    s_db_2 = 0.0\n",
    "\n",
    "\n",
    "    \n",
    "    #### starting gradient descent loop\n",
    "    start_time = time.time()\n",
    "    epoch_number = 1\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        \n",
    "        # selecting mini batch data for this iter\n",
    "        group_selected = np.mod(i, 6)\n",
    "        X = mini_batch_groups[group_selected]\n",
    "        Y = mini_batch_group_labels[group_selected]\n",
    "        if group_selected == 0:\n",
    "            epoch_number += 1\n",
    "            #print('end of epoch ' + str(i / 6))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # hidden layer: compute linear transformation of inputs\n",
    "        A_1_1, Z_1_1 = feedforward_relu_layer(X, W_1_1, B_1_1)\n",
    "        A_1_2, Z_1_2 = feedforward_relu_layer(A_1_1, W_1_2, B_1_2)\n",
    "        A_1_3, Z_1_3 = feedforward_relu_layer(A_1_2, W_1_3, B_1_3)\n",
    "\n",
    "\n",
    "        # output layer: linear transformation and activation func\n",
    "        Z_2 = np.dot(W_2, A_1_3) + B_2\n",
    "        Z_2 = Z_2.astype(np.float) # ensure is float; needed for sigmoid() to work\n",
    "        A_2 = softmax_activation(Z_2)\n",
    "        \n",
    "                \n",
    "        # store cost\n",
    "        m = A_2.shape[0] * A_2.shape[1]\n",
    "        #print(A_2[:4, :4])\n",
    "        loss_store[i] = -np.sum(Y * np.log(A_2)) / m  \n",
    "    \n",
    "\n",
    "\n",
    "        # output layer: get derivatives\n",
    "        dZ_2 = A_2 - Y   \n",
    "        print(dZ_2.shape)\n",
    "        dZ_2_temp = np.sum(dZ_2, axis = 0).reshape(1, 20)\n",
    "        print(dZ_2.shape)\n",
    "        dW_2 = (1/m) * (dZ_2_temp @ A_1_3.T)   #### issue is here: dims of dZ_2 =3,20; dims of A_1_3=10,20\n",
    "                                            ### however fudging this by creating dZ_2_temp doesnt help much\n",
    "                                            ### so something else is occuring\n",
    "                \n",
    "                                            ### weights are pushed towards zero, leading to what looks like\n",
    "                                            ### underflow: not clear on the cause\n",
    "        dB_2 = (1/m) * np.sum(dZ_2, axis=1, keepdims=True)  # sums horizontally\n",
    "        \n",
    "        print(dB_2.shape)\n",
    "\n",
    "        \n",
    "        \n",
    "        # hidden layers: get derivatives \n",
    "        dZ_1_3, dW_1_3, dB_1_3 = backpropagate_relu_layer(A_1_2, Z_1_3, dZ_2, W_2)\n",
    "        dZ_1_2, dW_1_2, dB_1_2 = backpropagate_relu_layer(A_1_1, Z_1_2, dZ_1_3, W_1_3)\n",
    "        dZ_1_1, dW_1_1, dB_1_1 = backpropagate_relu_layer(X, Z_1_1, dZ_1_2, W_1_2)\n",
    "\n",
    "        \n",
    "        # calculate decaying learning rate\n",
    "        lr = learning_rate_decay(lr_init, decay_rate, epoch_number)\n",
    "        \n",
    "        \n",
    "        # Updating Adam opimiser parameters\n",
    "        v_dw_1_1,v_db_1_1,s_dw_1_1, s_db_1_1 = update_adam_parameters(v_dw_1_1, v_db_1_1, s_dw_1_1, s_db_1_1, dW_1_1, dB_1_1, beta1, beta2)\n",
    "        v_dw_1_2,v_db_1_2,s_dw_1_2, s_db_1_2 = update_adam_parameters(v_dw_1_2, v_db_1_2, s_dw_1_2, s_db_1_2, dW_1_2, dB_1_2, beta1, beta2)\n",
    "        v_dw_1_3,v_db_1_3,s_dw_1_3, s_db_1_3 = update_adam_parameters(v_dw_1_3, v_db_1_3, s_dw_1_3, s_db_1_3, dW_1_3, dB_1_3, beta1, beta2)\n",
    "        v_dw_2,v_db_2,s_dw_2, s_db_2 = update_adam_parameters(v_dw_2, v_db_2, s_dw_2, s_db_2, dW_2, dB_2, beta1, beta2)\n",
    "\n",
    "                                                                \n",
    "         # leaving out 'bias correcting' v_dw and others, where t = total iterations so far\n",
    "        # v_dw = v_dw / (1 - beta^t)  \n",
    "        # v_db = v_db / (1 - beta^t)  \n",
    "        # s_dw = s_dw / (1 - beta2^t)\n",
    "        # s_db = s_db / (1 - beta2^t)\n",
    "\n",
    "        \n",
    "\n",
    "        # update weights using Adam optimiser scaling\n",
    "        \"\"\"\n",
    "        print('s_dw_1_1: ' + str(s_dw_1_1))\n",
    "        print('epsilon: ' + str(epsilon))\n",
    "        print('epsilon: ' + str(epsilon + s_dw_1_1))\n",
    "        print('np.sqrt(s_dw_1_1 + epsilon)')\n",
    "        print(type(s_dw_1_1))\n",
    "        print((s_dw_1_1.shape))\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        print('B_1_1 dim earlier: ' + str(B_1_1.shape))\n",
    "        print('v_db_1_1 dim earlier: ' + str(v_db_1_1.shape))\n",
    "        print('s_db_1_1 dim earlier: ' + str(s_db_1_1.shape))\n",
    "        \"\"\"\n",
    "        \n",
    "        W_1_1 = W_1_1 - (lr * v_dw_1_1) / np.sqrt(s_dw_1_1 + epsilon)\n",
    "        B_1_1 = B_1_1 - (lr * v_db_1_1) / np.sqrt(s_db_1_1 + epsilon)\n",
    "        \n",
    "        W_1_2 = W_1_2 - (lr * v_dw_1_2) / np.sqrt(s_dw_1_2 + epsilon)\n",
    "        B_1_2 = B_1_2 - (lr * v_db_1_2) / np.sqrt(s_db_1_2 + epsilon)\n",
    "        \n",
    "        W_1_3 = W_1_3 - (lr * v_dw_1_3) / np.sqrt(s_dw_1_3 + epsilon)\n",
    "        B_1_3 = B_1_3 - (lr * v_db_1_3) / np.sqrt(s_db_1_3 + epsilon)\n",
    "        \n",
    "        W_2 = W_2 - (lr * v_dw_2) / np.sqrt(s_dw_2 + epsilon)\n",
    "        B_2 = B_2 - (lr * v_db_2) / np.sqrt(s_db_2 + epsilon)\n",
    "                \n",
    "        if (np.mod(i, 1000) == 0):\n",
    "            print('iteration done: ' + str(i))\n",
    "\n",
    "            \"\"\"\n",
    "        W_1_1 = W_1_1 - lr * dW_1_1\n",
    "        W_1_2 = W_1_2 - lr * dW_1_2\n",
    "        W_1_3 = W_1_3 - lr * dW_1_3\n",
    "        B_1_1 = B_1_1 - lr * dB_1_1\n",
    "        B_1_2 = B_1_2 - lr * dB_1_2\n",
    "        B_1_3 = B_1_3 - lr * dB_1_3\n",
    "        W_2 = W_2 - lr * dW_2\n",
    "        B_2 = B_2 - lr * dB_2\n",
    "        \"\"\"\n",
    "        \n",
    "    \n",
    "    avg_iter_time = (time.time() - start_time) / iterations\n",
    "\n",
    "    return W_1_1, W_1_2, W_1_3, B_1_1,B_1_2, B_1_3, W_2, B_2, loss_store, avg_iter_time\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "postal-creature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "iteration done: 0\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "(3, 20)\n",
      "(3, 20)\n",
      "(3, 1)\n",
      "avg_iter_time: 0.0017972207069396973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-390-90dbb9431a34>:11: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(input_array) / np.sum(np.exp(input_array))\n",
      "<ipython-input-390-90dbb9431a34>:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  return np.exp(input_array) / np.sum(np.exp(input_array))\n",
      "<ipython-input-471-ec9875a37264>:94: RuntimeWarning: divide by zero encountered in log\n",
      "  loss_store[i] = -np.sum(Y * np.log(A_2)) / m\n",
      "<ipython-input-471-ec9875a37264>:94: RuntimeWarning: invalid value encountered in multiply\n",
      "  loss_store[i] = -np.sum(Y * np.log(A_2)) / m\n"
     ]
    }
   ],
   "source": [
    "## training model weights. n_layer represents the number of nodes in each layer\n",
    "W_1_1, W_1_2, W_1_3, B_1_1,B_1_2, B_1_3, W_2, B_2, loss_store, avg_iter_time = gradient_descent(train_input, \n",
    "                                                                                                train_labs, \n",
    "                                                                                                n_layer = 10, \n",
    "                                                                                                lr_init = 0.04, \n",
    "                                                                                                iterations = 100, \n",
    "                                                                                                decay_rate = 0.02)\n",
    "print('avg_iter_time: ' + str(avg_iter_time))\n",
    "# avg_iter_time of 0.005 seconds on full batch\n",
    "# avg_iter_time of 0.0012 - 0.0019 seconds on 1/6 minibatch\n",
    "# avg_iter_time of 0.00165 - 0.0032 on minibatch with Adam optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "behavioral-scoop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan]]\n",
      "[ 1.35379959  1.35845545  1.40075824  1.4853439   1.65642324  2.17004114\n",
      "  3.96865145  4.5863341   9.24327137 14.76260796         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan         nan         nan\n",
      "         nan         nan         nan         nan]\n"
     ]
    }
   ],
   "source": [
    "print(W_2)\n",
    "print(loss_store)\n",
    "\n",
    "\n",
    "# making labels in format for plot\n",
    "plot_y_vals = []\n",
    "for i in range(30):\n",
    "    if test_labs[0][i] == 1:\n",
    "        plot_y_vals.append(0)\n",
    "    if test_labs[1][i] == 1:\n",
    "        plot_y_vals.append(1)\n",
    "    if test_labs[2][i] == 1:\n",
    "        plot_y_vals.append(2)\n",
    "        \n",
    "\n",
    "#plt.plot([ 1.49508955 , 1.40739113  ,1.62602013,  2.16057992,  3.25374524,  4.95413588,12.07336694, 14.4747404,  32.348698])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "continuing-election",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x170e93cd0>]"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# viewing trend in loss\n",
    "plt.plot(loss_store)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "square-sentence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan]\n",
      " [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      "  nan nan nan nan nan nan nan nan nan nan nan nan]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# applying to test set (Z matrices are created but not used)\n",
    "A_1_1, Z_1_1 = feedforward_relu_layer(test_input, W_1_1, B_1_1)\n",
    "A_1_2, Z_1_2 = feedforward_relu_layer(A_1_1, W_1_2, B_1_2)\n",
    "A_1_3, Z_1_3 = feedforward_relu_layer(A_1_2, W_1_3, B_1_3)\n",
    "Z_2 = np.dot(W_2, A_1_3) + B_2\n",
    "print(Z_2)\n",
    "Z_2 = Z_2.astype(np.float) # ensure is float\n",
    "predictions = softmax_activation(Z_2)\n",
    "\n",
    "\n",
    "\n",
    "#### plotting predictions vs actual for the three classes\n",
    "plot_predictions = []\n",
    "for i in range(30):\n",
    "    val = np.argmax([predictions[0][i], predictions[1][i], predictions[2][i]])\n",
    "    plot_predictions.append(val)\n",
    "\n",
    "plt.scatter(plot_predictions, plot_y_vals)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-emission",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-assistant",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spiritual-fellowship",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
