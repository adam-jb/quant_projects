{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alert-boring",
   "metadata": {},
   "source": [
    "\n",
    "Reading pytest docs could be helpful: https://docs.pytest.org/en/latest/example/index.html\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hispanic-election",
   "metadata": {},
   "source": [
    "\n",
    "## Notes from https://www.youtube.com/watch?v=eAPmXQ0dC7Q&ab_channel=WesDoyle\n",
    "\n",
    "Process: write test (that will fail), then write just enough code to pass the test\n",
    "\n",
    "around minute 12-15, makes a few lines of code to load all files as a package\n",
    "\n",
    "Could perhaps do the below without using OOP. Could just test that each new thing made is of the desired type, shape, class, etc. Could write tests for that as a function before making the thing: example of doing this in test_add_func() below. \n",
    "\n",
    "I think functions could be stress tested for generating unusual data. Such a test could be functionalised and defined before making the tested function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "continental-theater",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test: in the video running this would run tests; doesn't do that here\n",
    "import unittest\n",
    "\n",
    "\n",
    "class TestClient(unittest.TestCase):\n",
    "    \n",
    "    def test_get_ents_returns_dictionary_given_empty_string(self):\n",
    "        ner = NamedEntityClient()\n",
    "        ents = ner.get_ents()\n",
    "        self.assertIsInstance(ents, dict)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "alpine-facial",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TestClient testMethod=runTest>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TestClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "grave-telling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# thing to make, which makes test pass (this would be in another script, which would be sourced by the test script)\n",
    "class NamedEntityClient():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-hundred",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "north-ceiling",
   "metadata": {},
   "source": [
    "## Notes from https://www.youtube.com/watch?v=etosV2IWBF0&ab_channel=MattLayman\n",
    "\n",
    "#### Pytest presentation\n",
    "\n",
    "pytest = most popular testing package\n",
    "\n",
    "assert() is the basis for many functions (eg: assertTrue, assertIs, assertNot, etc)\n",
    "\n",
    "can run 'pytest' as a CLI tool\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "subtle-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fiscal-vegetarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "assert 1 == a  # will throw error if fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "becoming-highland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_add_func():\n",
    "    d = add(2, 3)\n",
    "    assert d == 5\n",
    "    assert add(2, 9) == 11\n",
    "    assert str(type(add(2, 9))) == \"<class 'int'>\"\n",
    "        \n",
    "    with pytest.raises(TypeError): # Not right: TypeError context manager somehow prevents line below from failing\n",
    "        add(2, '11')\n",
    "        \n",
    "    for i in range(100):\n",
    "        assert add(3, i) == i + 3\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "pregnant-personality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def func and run it's corresponding test\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "test_add_func()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "answering-donor",
   "metadata": {},
   "outputs": [
    {
     "ename": "CalcError",
     "evalue": "error message on fail: please ensure all inputs are int or float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-af0c1cdd3a59>\u001b[0m in \u001b[0;36madd_type_catch\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mCalcError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-af0c1cdd3a59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mCalcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error message on fail: please ensure all inputs are int or float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0madd_type_catch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-79-af0c1cdd3a59>\u001b[0m in \u001b[0;36madd_type_catch\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mCalcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"error message on fail: please ensure all inputs are int or float\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0madd_type_catch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"a\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalcError\u001b[0m: error message on fail: please ensure all inputs are int or float"
     ]
    }
   ],
   "source": [
    "### Exception handling in a func (there is no testing in this cell)\n",
    "class CalcError(Exception):\n",
    "    \"\"\"An exception class\"\"\"\n",
    "\n",
    "def add_type_catch(a, b):\n",
    "    try:\n",
    "        return a + b\n",
    "    except TypeError:\n",
    "        raise CalcError(\"error message on fail: please ensure all inputs are int or float\")\n",
    "\n",
    "add_type_catch(1, \"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "leading-think",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Type is <class 'int'>\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " f\"Type is {type(add(2, 9))}\"    # put it in an fstring. No purpose just for interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-mexico",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "returning-florida",
   "metadata": {},
   "source": [
    "## Notes from https://www.youtube.com/watch?v=fv259R38gqc&ab_channel=MattLayman\n",
    "\n",
    "Part 2 of the previous section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "olympic-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "### as above, with decorator to put multiple inputs into tests\n",
    "\n",
    "@pytest.mark.parametrize(\n",
    "    'a, b, expected', [\n",
    "        (1, 1, 2),\n",
    "        (1, 2, 3),\n",
    "        (2, 1, 3),\n",
    "        (3, 1, 4)\n",
    "    ]\n",
    ")\n",
    "def test_add_func(a, b, expected):\n",
    "    d = add(a, b)\n",
    "    assert d == expected\n",
    "\n",
    "## in video this runs when a py script of this is run. Not sure how to run it in Jupyter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-pakistan",
   "metadata": {},
   "source": [
    "classes can be good containers of tests\n",
    "\n",
    "fixtures = provides a defined, reliable and consistent context for the tests. This could include environment (for example a database configured with known parameters) or content (such as a dataset). (source https://docs.pytest.org/en/latest/explanation/fixtures.html) \n",
    "\n",
    "make a function a fixture (or assign it to a fixture or something like that) with decorator @pytest.fixture\n",
    "\n",
    "pytest has some built in fixtures you can use\n",
    "\n",
    "says pytest is very versatile (lots of modules, similar to idea flask I think): people have used it for testing hardware\n",
    "\n",
    "(up to 32 mins)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greek-first",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-brick",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compressed-parameter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "guilty-integrity",
   "metadata": {},
   "source": [
    "### Notes from https://www.youtube.com/watch?v=B1j6k2j2eJg&ab_channel=ArjanCodes\n",
    "\n",
    "red, green refactor: make failing test, make code pass test, refactor code while checking it still passes tests\n",
    "\n",
    "can set up VSCode to write and run test and main separate scripts quickly\n",
    "\n",
    "says that if you're likely to change loads about your code (ie, the expected halflife of the code is very low), perhaps because you're in an early stage startup, the extra time to write loaadddss of tests might not be worth it. Have to judge for yourself: there is a tradeoff between speed (including learning things about product faster) and well-tested-ness.\n",
    "\n",
    "there is a risk that tests give a false sense of security\n",
    "\n",
    "tests might not protect you against people using a feature in the wrong way\n",
    "\n",
    "want tests to be independent from each other if possible (Eg: different input data for each test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clear-concentrate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-possible",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-hybrid",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-hacker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designing-above",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hundred-recovery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-sleeve",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "manufactured-assault",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-overall",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-bishop",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interpreted-lloyd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-catalyst",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sitting-luther",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
