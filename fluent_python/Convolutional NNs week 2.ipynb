{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dirty-proceeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Notes from week 2 of https://www.coursera.org/learn/convolutional-neural-networks/home/week/2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# says if a certain NN architecture works well on one task, its likely to do well on other tasks\n",
    "# so good to be familiar with some effective existing architectures\n",
    "\n",
    "# LeNet-5: conv/maxPool2/conv/maxPool2/FC/FC/output_with_softmax (60k params overall)\n",
    "\n",
    "# c = channel count, s = stride, w = width of conv matrix in pixels\n",
    "# AlexNet: 227px wide/conv(w=11,s=5,c=96)/maxpool(w=3,s=2)/conv(w=5,s=1,c=256)/maxpool(w=3,s=2)/\n",
    "#          conv(w=5,s=1,c=256)/conv(w=5,s=1,c=256)/conv(w=5,s=1,c=256)/ [same conv layers x3 times here]\n",
    "#.         maxpool(w=3,s=2)/unravel to 9216 features (6*6*256)/\n",
    "#          FC(9216->4096) / FC(4096->4096) / FC(4096->1000)output_with_softmax\n",
    "# 60million params in total\n",
    "# Takes 227*227*3 images\n",
    "# originally trained on 1.2m images of 1000 classes\n",
    "# pre-trained AlexNet is available as a PyTorch file: https://www.kaggle.com/pytorch/alexnet\n",
    "\n",
    "# VGG-16: deeper network, using padding so dimensions are same after each conv layer \n",
    "#    dims get smaller with each pool tho\n",
    "#.   all conv layers are 3*3, stride = 1, channels count denoted by square bracket below\n",
    "#.    all pool layers are 2*2, stride = 2\n",
    "#    where identical layers are repeated, denoted as x2/x3/etc below\n",
    "# 224/3 (224*224*3) start/\n",
    "#conv[64]x2/pool   (112/64)\n",
    "# conv[128]x2/pool  (56/128)\n",
    "# conv[256]x3/pool/  (28/256)\n",
    "# conv[512]x3/pool/  (14/512)\n",
    "# conv[512]x3/pool   (7/512)\n",
    "# FC(7*7*512 in, 4096 out)\n",
    "# FC (4096 in, 4096 out)\n",
    "# FC with softmax output (4096 in, 1000 out)\n",
    "### 138million weights\n",
    "\n",
    "# VGG-19: newer version of VGG-16, which performs slightly better\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pointed-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNets are made of \"residual blocks\"\n",
    "\n",
    "# a residual block is two FC layers with relu activation, but the data that feeds into the first layer\n",
    "# is preserved, and added to the input of the 2nd relu activation, looks like:\n",
    "\n",
    "# resnet method can be applied to any layer type (eg: convolutions)\n",
    "# not sure if activation is always relu. \n",
    "# structure:\n",
    "# input -> layer(W1) -> relu -> layer(W2) -> add input to this -> relu \n",
    "\n",
    "# stack lots of residual blocks to make network\n",
    "# says using residual blocks lets you make much deeper networks\n",
    "\n",
    "# with normal NNs the loss on the training set can increase as you add layers past a certain point (if too deep)\n",
    "# resnets only get better with more depth (on the training set at least)\n",
    "# how? perhaps to do with taking untransformed data forward, so giving NN a better 'memory' of old layers\n",
    "# res blocks give NN the option of effectively making no change in a layer (eg: if weights are zero\n",
    "# then it just applies relu to a dataset that's already been relu'd)\n",
    "\n",
    "# dimensions tend not to change within the residual block\n",
    "\n",
    "# for good diagram on resnets:\n",
    "# https://www.coursera.org/learn/convolutional-neural-networks/lecture/HAhz9/resnets\n",
    "\n",
    "# why resnets work:\n",
    "# https://www.coursera.org/learn/convolutional-neural-networks/lecture/XAKNO/why-resnets-work\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "systematic-valley",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1x1 convolution: if a 2d image just multiplies each value by the 1x1's value\n",
    "\n",
    "# if image has >1 channels, aggregates by channels multiplying channel values with\n",
    "# 1x1xchannel_count, then sum and put through relu\n",
    "\n",
    "# eg: so if have 6x6x10 image, have 1x1x3 multiplier, for each 3rd-dim line (len=10) in the 6x6 grid,\n",
    "# sum those values multiplied by the 1x1x1 value in the pooling multiplier, then do relu on them, \n",
    "# giving 6x6 grid. \n",
    "# then do this 3 times (once for each value in 1x1). Each iter will give a 6x6 output, so a 1x1x3 multiplier\n",
    "# gives a 6x6x3 output\n",
    "\n",
    "# so a good way to reduce 5x5x200 image to 5x5x40 without losing much information might be \n",
    "# to put it through a 1x1x40 \"1x1 conv\" layer \n",
    "\n",
    "# sometimes called 'network in network'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aerial-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inception network\n",
    "\n",
    "# does multiple conv operations on the same input, ensuring they all output the same 2d dimensions, and\n",
    "# stacking the operation outputs channel-wise. the operations include conv layers with different sized\n",
    "# conv matrices and pooling\n",
    "\n",
    "# vs 5x5 conv matrix, 1x1 conv matrix is about one tenth the computational cost. so better to do 1x1 (it reduces dims)\n",
    "# before the 5x5, all else being equal\n",
    "# adding a 1x1 conv layer prior to a 5x5 to improve performance is called adding a 'bottleneck' layer\n",
    "# says doing this tends not to hurt performance (and performance cost remains at about 1/10 the cost of \n",
    "# just the 5x5 layer, even though you're processing 2 layers instead of 1)\n",
    "\n",
    "\n",
    "# inception module might look like: input from previous layer goes through all of these in parallel:\n",
    "# 1x1 conv -> 3x3 conv\n",
    "# 1x1 conv -> 5x5 conv\n",
    "# 1x1 conv\n",
    "# maxpool, s=1, output dim = same -> 1x1 conv (to reduce number of channels otherwise pooling layer channels\n",
    "                # could dominate)\n",
    "## stack all 4 outputs channel-wise\n",
    "\n",
    "\n",
    "\n",
    "# inception network is made up blocks of inception modules (sandwiched by a few beginning and\n",
    "# end-stage processing layers)\n",
    "# also can have sidebranches, which take a layer's output and put it through FC and softmax to make\n",
    "# a prediction (can see how prediction accuracy changes at different layers of the network, and\n",
    "# check weights of intermediate layers arent hugely twisting the inputs - ie, the results should\n",
    "# be roughly on track if not performing as well as the final output layer)\n",
    "\n",
    "\n",
    "# other versions of inception have now been made (inception2, 3, etc) and in combination with resNets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aging-symposium",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3111111111111111"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## MobileNet aka Depthwise Separable Convolution\n",
    "\n",
    "# for use with little compute - so can be applied on phones\n",
    "# \n",
    "\n",
    "## Process\n",
    "# 1) for each channel of input image, apply a separate conv matrix (each channel is treated separately\n",
    "# with it's own conv matrix). Returns a slightly smaller image (if no padding) with same channel count\n",
    "# 2) apply 'pointwise' conv (aka 'projection'), involving 1x1xchannel_count multiplied by each square in the input\n",
    "# tensor (so each 'front square' gets 3 multiplications which are summed, if there are 3 channels)\n",
    "# This returns a 2d image of same dims as input at start of (2). Can do this step multiple times \n",
    "# with different 1x1xchannel_count inputs, each returning a new output channel (making the output\n",
    "# to this step 3d)\n",
    "# \n",
    "# these 2 processes are called Depthwise and Pointwise. Combined they are DepthWise Separable Convolution\n",
    "# and doing these can save a lot of compute vs doing a usual convolution\n",
    "# \n",
    "\n",
    "# costs of doing this vs standard conv can be estimated with a formula:\n",
    "def mobilenet_cost_vs_conv(channel_count, conv_matrix_size):\n",
    "    return (1 / channel_count) + (1 / conv_matrix_size**2)\n",
    "\n",
    "mobilenet_cost_vs_conv(5, 3) # the coursera example. \n",
    "# Says IRL this number will be smaller - maybe around 0.1\n",
    "\n",
    "\n",
    "\n",
    "# mobilenet explanation:\n",
    "# https://www.coursera.org/learn/convolutional-neural-networks/lecture/B1kPZ/mobilenet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MobileNet architecture:\n",
    "# 13 repeated layers of Depthwise Separable Convolution, following by pooling and output softmax\n",
    "\n",
    "\n",
    "# MobileNetv2 architecture:\n",
    "# adds \n",
    "# 1) \"residual connection\": taking output from one layer and passing it directly to output of the next - I\n",
    "# think it stacks channel-wise or maybe aggregates into activation func at end of conv layer (similar to resNet)\n",
    "# 2) adds 'expansion layer' prior to depthwise or pointwise operations: replicating input channel-wise N\n",
    "# times. eg if N=6, 4x4x3 input becomes 4x4x18\n",
    "\n",
    "# expansion layer + depthwise layer + pointwise (aka projection) layer = 'bottleneck block' (when all 3 combined)\n",
    "\n",
    "# v2 has 17 bottleneck blocks, then pooling and output softmax\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-visit",
   "metadata": {},
   "outputs": [],
   "source": [
    "## things you can do to change where you sit on compute/performance scale:\n",
    "# change image resolution\n",
    "# make network deeper\n",
    "# increase 'width' of network (number of operations happening on each layer: so more nodes in FC layers\n",
    "# and bigger outputs from conv layers)\n",
    "\n",
    "# EfficientNet provides process for finding best tradeoff and combination of these for your implementation\n",
    "# inc your hardware limitations. Not 100% this is correct as it's also available as a pre-trained network\n",
    "# itself:\n",
    "# https://github.com/lukemelas/EfficientNet-PyTorch\n",
    "\n",
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# can be hard to replicate exact model architecture based on papers: if they have github code\n",
    "# good to start with that, rather than from scratch\n",
    "\n",
    "\n",
    "# if you're trying something new, look for existing work on similar tasks, and borrow code creating \n",
    "# network which is known to work on for that\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-funds",
   "metadata": {},
   "outputs": [],
   "source": [
    "### transfer learning:\n",
    "# get rid of existing output layer (which will often be softmax), and make your own\n",
    "# with the classifications you want (eg: photo of anya, dad, or someone else)\n",
    "# \n",
    "# think of weights for all layers apart from output layer as frozen (might have to manually freeze these\n",
    "# in pytorch using some parameter/code)\n",
    "#\n",
    "# put all inputs in your training data through frozen layers, saving output of final frozen layer\n",
    "# then train a shallow softmax model to predict result, based on input from this final frozen layer, \n",
    "# \n",
    "# if you have a larger training set, you could unfreeze some of the later layers (if do this have\n",
    "# option of using existing weights as initial weights, or can start with random weights)\n",
    "\n",
    "# rule of thumb: the more data you have the more layers you might unfreeze\n",
    "\n",
    "# says you should almost always do transfer learning if doing computer vision, unless you have tonnes\n",
    "# of resources\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-slovak",
   "metadata": {},
   "outputs": [],
   "source": [
    "## data augmentation methods\n",
    "# mirroring\n",
    "# random cropping\n",
    "# rotation\n",
    "# shearing\n",
    "# local warping\n",
    "# color shifting (adding or subtracting randomly to RGB channels, often by small amounts drawn from distributions)\n",
    "#.  'PCA colour augmentation' is a method using PCA to scale RGB in a way that keeps tints representative of the real\n",
    "        # world\n",
    "#     \n",
    "\n",
    "\n",
    "\n",
    "## where training set doesnt fit in memory, might have a streaming process that loads one mini-batch's worth \n",
    "# of data from HDD, applies distortions if needed, then feeds it through training iteration on gradient descent\n",
    "# and so on\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "## tips for doing well on benchmarks:\n",
    "# ensembling (tends not to be used in production as it might not be worth the extra compute)\n",
    "# Multi-crop (at test time): run classifier on multiple versions of test images and average the results, eg:\n",
    "    # images might be mirrored and randomly cropped . '10 crop' is one popular one\n",
    "    # Also tends not to be used in production, though far less expensive than ensembling\n",
    "# \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
