{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "surprised-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# designing and implementing CNN in PyTorch to classify fashionMNIST\n",
    "\n",
    "# accuracy = 9% # Worse than guessing! (And worse than the 35% from feed forward NN) \n",
    "# perhaps the already-low resolution of fashionMNIST doesnt benefit from CNN\n",
    "\n",
    "# datasets available via tensorflow ('beans' looks fun for determining crop health)\n",
    "# https://www.tensorflow.org/datasets/catalog/overview\n",
    "\n",
    "\n",
    "## not used here. Ray tune seems a great way to tune hyperparameters\n",
    "# https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html\n",
    "from ray import tune\n",
    "from ray.tune import CLIReporter\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "\n",
    "\n",
    "# to find cutting edge algos to draw inspiration from, see this list:\n",
    "# https://paperswithcode.com/sota/image-classification-on-fashion-mnist\n",
    "\n",
    "\n",
    "# Hadamard product = element wise multiplication\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from torch import nn, rand, sum\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda, Compose\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor    # ToTensor converts PIL image or nparray to FloatTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "confused-relations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1896, 0.2594, 0.7023, 0.0846, 0.5693, 0.1691, 0.3450, 0.9565, 0.2998,\n",
      "         0.7538]])\n",
      "tensor([[0.0753, 0.0807, 0.1257, 0.0678, 0.1101, 0.0738, 0.0880, 0.1621, 0.0841,\n",
      "         0.1324]])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "X = rand(1, 10)\n",
    "print(X)                      # random vector of 10 values, which could be NN outputs for 10-class classifier\n",
    "print(nn.Softmax(dim=1)(X))   # may wish to put outputs of NN through softmax\n",
    "print(sum(nn.Softmax(dim=1)(X)))   # showing softmax outputs sum to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "colonial-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "certified-throw",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 60000\n",
       "    Root location: data\n",
       "    Split: Train\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "compatible-warning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see first 3 images in training data set\n",
    "plt.subplot(1,3,1)    \n",
    "plt.imshow(training_data[0][0][0, :, :], cmap=\"gray\")\n",
    "plt.subplot(1,3,2)    \n",
    "plt.imshow(training_data[1][0][0, :, :], cmap=\"gray\")\n",
    "plt.subplot(1,3,3)    \n",
    "plt.imshow(training_data[2][0][0, :, :], cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "wired-andrew",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y:  torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64 # minibatch size\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size) # creates python iterable over dataset\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "turkish-mounting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.0\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "def get_height_or_width_output(n_previous_layer, padding, filter_size, stride):\n",
    "    \"\"\"This func can apply to both width or height of matrix\n",
    "    Returns dimensions of output layer from input layer\n",
    "    \"\"\"\n",
    "    return ( (n_previous_layer + 2*padding - filter_size)/ stride) + 1\n",
    "\n",
    "\n",
    "#### Using this formula to work out what dimensions I should set the layers to when defining the model\n",
    "# nn.Conv2d defaults: stride=1, padding=0; \n",
    "# filter_size aka kernel_size (width of convolution matrix)\n",
    "print(get_height_or_width_output(28, 0, 3, 1) )\n",
    "print(get_height_or_width_output(26, 0, 3, 1) / 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "satisfied-junction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "NeuralNetwork(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=2304, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)  # outputs 24*24*5 square (12*12 after pooling)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3) # outputs 8*8*16 square (4*4 after pooling)\n",
    "        self.fc1 = nn.Linear(16 * 12 * 12, 120)  #4*4*16=256=total flattened features from previous Conv matrix\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)        # outputs 10 values\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 1)  # max pool essentially doing nothing when set to 1\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)  # max pool reduces both dimensions of square in half \n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  \n",
    "        x = nn.Softmax(dim=1)(x)  # scales outputs vector to sum to 1\n",
    "        return x\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "indirect-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()     # chosing loss function \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)  # set optimiser with learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aware-sustainability",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    \"\"\"Single training epoch, looping through mini-batches as set in dataloader obj\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()  # setting weights to zero - not sure if doing this on every epoch or not\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:     # printing diagnostics on every 100th batch\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "usual-copper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    \"\"\"test performance of model on test dataset\"\"\"\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()    # puts the model into eval model (instead of train); a form of switch\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():    # no_grad disables gradient calculation\n",
    "                               # anytime we're applying the model, put it within no_grad so no updates made\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)         \n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches   # shorthand for: test_loss = test_loss / num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "sapphire-branch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.302513  [    0/60000]\n",
      "loss: 2.302863  [ 6400/60000]\n",
      "loss: 2.302308  [12800/60000]\n",
      "loss: 2.302303  [19200/60000]\n",
      "loss: 2.303178  [25600/60000]\n",
      "loss: 2.303082  [32000/60000]\n",
      "loss: 2.303248  [38400/60000]\n",
      "loss: 2.302658  [44800/60000]\n",
      "loss: 2.302197  [51200/60000]\n",
      "loss: 2.304625  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.302734 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.302351  [    0/60000]\n",
      "loss: 2.302719  [ 6400/60000]\n",
      "loss: 2.302102  [12800/60000]\n",
      "loss: 2.302131  [19200/60000]\n",
      "loss: 2.303038  [25600/60000]\n",
      "loss: 2.302890  [32000/60000]\n",
      "loss: 2.303108  [38400/60000]\n",
      "loss: 2.302491  [44800/60000]\n",
      "loss: 2.302042  [51200/60000]\n",
      "loss: 2.304449  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.302559 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.302193  [    0/60000]\n",
      "loss: 2.302575  [ 6400/60000]\n",
      "loss: 2.301896  [12800/60000]\n",
      "loss: 2.301966  [19200/60000]\n",
      "loss: 2.302899  [25600/60000]\n",
      "loss: 2.302711  [32000/60000]\n",
      "loss: 2.302967  [38400/60000]\n",
      "loss: 2.302336  [44800/60000]\n",
      "loss: 2.301904  [51200/60000]\n",
      "loss: 2.304282  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.302395 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.302055  [    0/60000]\n",
      "loss: 2.302443  [ 6400/60000]\n",
      "loss: 2.301711  [12800/60000]\n",
      "loss: 2.301810  [19200/60000]\n",
      "loss: 2.302757  [25600/60000]\n",
      "loss: 2.302541  [32000/60000]\n",
      "loss: 2.302828  [38400/60000]\n",
      "loss: 2.302191  [44800/60000]\n",
      "loss: 2.301774  [51200/60000]\n",
      "loss: 2.304120  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.302241 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.301933  [    0/60000]\n",
      "loss: 2.302321  [ 6400/60000]\n",
      "loss: 2.301543  [12800/60000]\n",
      "loss: 2.301665  [19200/60000]\n",
      "loss: 2.302615  [25600/60000]\n",
      "loss: 2.302383  [32000/60000]\n",
      "loss: 2.302689  [38400/60000]\n",
      "loss: 2.302054  [44800/60000]\n",
      "loss: 2.301656  [51200/60000]\n",
      "loss: 2.303969  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 10.0%, Avg loss: 2.302097 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "### training process\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)   \n",
    "    test(test_dataloader, model, loss_fn)   # tells you how it's doing\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-notification",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "comparative-static",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "foreign-relief",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "9\n",
      "tensor([[0.0950, 0.1118, 0.1027, 0.1081, 0.0982, 0.1043, 0.0951, 0.0900, 0.1005,\n",
      "         0.0943]])\n",
      "class with highest prediction: Trouser\n",
      "Predicted: \"Trouser\", Actual: \"Ankle boot\"\n"
     ]
    }
   ],
   "source": [
    "# making a prediction for a single data point\n",
    "classes = [\n",
    "    \"T-shirt/top\",\n",
    "    \"Trouser\",\n",
    "    \"Pullover\",\n",
    "    \"Dress\",\n",
    "    \"Coat\",\n",
    "    \"Sandal\",\n",
    "    \"Shirt\",\n",
    "    \"Sneaker\",\n",
    "    \"Bag\",\n",
    "    \"Ankle boot\",\n",
    "]\n",
    "\n",
    "model.eval()    # eval mode is also prediction mode\n",
    "x, y = test_data[1][0], test_data[0][1]\n",
    "print(x.shape)\n",
    "print(y)\n",
    "softmax = nn.Softmax(dim=1)  # defining softmax for our uses\n",
    "with torch.no_grad():    # anytime we're applying the model, put it within no_grad so no updates made\n",
    "    pred = model(x[None, ...])   # model(x) doesn't work, need [None,...] too \n",
    "                                # because pytorch expects a batch, not a single value\n",
    "                                # https://stackoverflow.com/questions/57237381/runtimeerror-expected-4-dimensional-input-for-4-dimensional-weight-32-3-3-but\n",
    "    print(pred)\n",
    "    print('class with highest prediction: ' + str(classes[pred[0].argmax(0)]))\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "composite-sender",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "robust-workshop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv1.weight | Size: torch.Size([6, 1, 3, 3]) | Values : tensor([[[[-0.1057,  0.0530, -0.1202],\n",
      "          [ 0.2602,  0.0943, -0.1224],\n",
      "          [ 0.0051,  0.1386, -0.1116]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1582,  0.1826, -0.0364],\n",
      "          [ 0.0590,  0.2801,  0.0622],\n",
      "          [-0.0244,  0.2316, -0.1661]]]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: conv1.bias | Size: torch.Size([6]) | Values : tensor([-0.3077, -0.1342], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: conv2.weight | Size: torch.Size([16, 6, 3, 3]) | Values : tensor([[[[-0.1352, -0.1257, -0.0583],\n",
      "          [-0.0585,  0.1126, -0.1161],\n",
      "          [-0.0823, -0.1116,  0.1002]],\n",
      "\n",
      "         [[ 0.0469, -0.0897, -0.0770],\n",
      "          [ 0.0393, -0.1159,  0.1216],\n",
      "          [ 0.0918, -0.0914, -0.1337]],\n",
      "\n",
      "         [[ 0.1260, -0.0190, -0.0460],\n",
      "          [-0.0173,  0.0201, -0.0485],\n",
      "          [-0.0578,  0.1249, -0.0157]],\n",
      "\n",
      "         [[-0.0161,  0.0242,  0.1189],\n",
      "          [-0.0871,  0.1346,  0.1307],\n",
      "          [-0.1270,  0.0803, -0.1210]],\n",
      "\n",
      "         [[-0.0997,  0.1251, -0.0612],\n",
      "          [-0.0420,  0.1136,  0.0429],\n",
      "          [-0.0434, -0.1194, -0.1164]],\n",
      "\n",
      "         [[ 0.1319, -0.0453,  0.0724],\n",
      "          [-0.0997,  0.0448,  0.1322],\n",
      "          [ 0.1127, -0.0613, -0.0862]]],\n",
      "\n",
      "\n",
      "        [[[-0.0028, -0.0564, -0.0477],\n",
      "          [ 0.1272, -0.0412,  0.1219],\n",
      "          [-0.0773, -0.0687,  0.0980]],\n",
      "\n",
      "         [[ 0.1192,  0.1160, -0.0502],\n",
      "          [-0.0172,  0.1004,  0.0749],\n",
      "          [-0.1229, -0.0869, -0.0121]],\n",
      "\n",
      "         [[ 0.0608, -0.0861, -0.0596],\n",
      "          [-0.0946,  0.0544,  0.1326],\n",
      "          [ 0.0388, -0.0834,  0.1177]],\n",
      "\n",
      "         [[-0.0106, -0.0401,  0.0142],\n",
      "          [ 0.0268, -0.0762,  0.0345],\n",
      "          [ 0.0897, -0.0558, -0.1195]],\n",
      "\n",
      "         [[-0.0551,  0.0296,  0.1142],\n",
      "          [-0.0216, -0.1361, -0.0141],\n",
      "          [-0.1255,  0.1203, -0.0186]],\n",
      "\n",
      "         [[ 0.1215, -0.0727, -0.0279],\n",
      "          [ 0.0681, -0.0548,  0.0153],\n",
      "          [ 0.0927,  0.0665,  0.0835]]]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: conv2.bias | Size: torch.Size([16]) | Values : tensor([0.0400, 0.1047], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: fc1.weight | Size: torch.Size([120, 2304]) | Values : tensor([[ 0.0088, -0.0014,  0.0139,  ..., -0.0207,  0.0145,  0.0035],\n",
      "        [ 0.0095, -0.0114, -0.0192,  ...,  0.0037,  0.0064, -0.0189]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: fc1.bias | Size: torch.Size([120]) | Values : tensor([ 0.0063, -0.0197], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: fc2.weight | Size: torch.Size([84, 120]) | Values : tensor([[ 0.0533, -0.0634,  0.0723, -0.0773,  0.0740,  0.0066,  0.0588, -0.0805,\n",
      "         -0.0838, -0.0486, -0.0472, -0.0752, -0.0665, -0.0009, -0.0822, -0.0485,\n",
      "          0.0737,  0.0661,  0.0418, -0.0534,  0.0649,  0.0602, -0.0743,  0.0847,\n",
      "          0.0084, -0.0037, -0.0532, -0.0741,  0.0388, -0.0064,  0.0390,  0.0380,\n",
      "          0.0171,  0.0849,  0.0049,  0.0115,  0.0293, -0.0367, -0.0835, -0.0081,\n",
      "         -0.0705, -0.0326,  0.0574,  0.0892, -0.0510, -0.0706, -0.0526,  0.0439,\n",
      "          0.0317,  0.0353,  0.0172,  0.0804, -0.0890, -0.0190,  0.0289, -0.0015,\n",
      "          0.0776, -0.0823,  0.0447,  0.0120,  0.0306,  0.0559,  0.0376,  0.0043,\n",
      "         -0.0071, -0.0863, -0.0468,  0.0291, -0.0251,  0.0705, -0.0823, -0.0396,\n",
      "          0.0395,  0.0293, -0.0505,  0.0124, -0.0591, -0.0047, -0.0157,  0.0865,\n",
      "          0.0870, -0.0571,  0.0229,  0.0038, -0.0750,  0.0169, -0.0133, -0.0734,\n",
      "         -0.0095,  0.0010, -0.0088,  0.0818, -0.0905, -0.0784,  0.0756,  0.0195,\n",
      "          0.0490,  0.0497, -0.0617, -0.0512, -0.0773, -0.0337,  0.0721,  0.0332,\n",
      "          0.0849, -0.0674, -0.0210,  0.0168, -0.0034,  0.0298,  0.0127, -0.0284,\n",
      "          0.0426, -0.0224,  0.0516, -0.0589,  0.0742, -0.0428, -0.0021,  0.0138],\n",
      "        [ 0.0021, -0.0246,  0.0120, -0.0090,  0.0758, -0.0694, -0.0353,  0.0256,\n",
      "         -0.0744,  0.0685,  0.0449, -0.0554, -0.0290,  0.0638,  0.0379, -0.0887,\n",
      "         -0.0338,  0.0218,  0.0522,  0.0093, -0.0619,  0.0117,  0.0243,  0.0344,\n",
      "          0.0323, -0.0140,  0.0090, -0.0577,  0.0266,  0.0411, -0.0646, -0.0664,\n",
      "         -0.0414,  0.0157,  0.0245,  0.0595, -0.0417, -0.0508,  0.0400, -0.0035,\n",
      "          0.0808,  0.0655,  0.0399,  0.0182,  0.0550, -0.0307, -0.0023,  0.0026,\n",
      "         -0.0493, -0.0151,  0.0885, -0.0852,  0.0860, -0.0207, -0.0450,  0.0840,\n",
      "          0.0286, -0.0377, -0.0906,  0.0368, -0.0414,  0.0301, -0.0734,  0.0664,\n",
      "          0.0456,  0.0128, -0.0383, -0.0636, -0.0071,  0.0718,  0.0460, -0.0033,\n",
      "          0.0708, -0.0781, -0.0674,  0.0268,  0.0602, -0.0349, -0.0590, -0.0743,\n",
      "         -0.0854,  0.0505, -0.0282,  0.0090,  0.0762, -0.0448, -0.0600, -0.0086,\n",
      "          0.0442,  0.0544,  0.0623, -0.0070,  0.0173, -0.0433, -0.0420, -0.0327,\n",
      "          0.0250, -0.0487, -0.0617,  0.0509,  0.0827,  0.0739, -0.0699,  0.0658,\n",
      "         -0.0236,  0.0091, -0.0400, -0.0445,  0.0191, -0.0611,  0.0404, -0.0002,\n",
      "         -0.0431,  0.0103, -0.0204, -0.0035,  0.0045, -0.0374, -0.0639, -0.0429]],\n",
      "       grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: fc2.bias | Size: torch.Size([84]) | Values : tensor([-0.0013, -0.0435], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: fc3.weight | Size: torch.Size([10, 84]) | Values : tensor([[-0.0328, -0.0793, -0.0751,  0.0921,  0.0731, -0.1044,  0.0359, -0.0950,\n",
      "          0.0724, -0.0740,  0.0549,  0.0059,  0.0786, -0.0587,  0.0263,  0.0509,\n",
      "          0.0255, -0.0743, -0.0370,  0.0389, -0.0768, -0.0565,  0.0258,  0.0836,\n",
      "          0.0036,  0.0328,  0.1051, -0.0855, -0.0508, -0.1034, -0.0022, -0.0512,\n",
      "         -0.0392, -0.1088,  0.0178, -0.0796, -0.0238,  0.0657, -0.0791, -0.0759,\n",
      "         -0.0280,  0.0603,  0.0284,  0.0165,  0.0198, -0.1078, -0.0235, -0.0447,\n",
      "         -0.0466, -0.0900,  0.0884,  0.0505,  0.0695, -0.0790,  0.0106,  0.0896,\n",
      "          0.0025,  0.0731,  0.0661, -0.0830, -0.0873, -0.0471, -0.0699,  0.0282,\n",
      "         -0.0506,  0.0755, -0.0940,  0.1083, -0.0931,  0.0025, -0.0390, -0.0842,\n",
      "         -0.0464,  0.0991, -0.0909, -0.0062, -0.0409, -0.0728, -0.0507, -0.0194,\n",
      "         -0.1087,  0.0746,  0.0140,  0.0221],\n",
      "        [ 0.0625, -0.0399,  0.0501,  0.0517,  0.0709,  0.0449,  0.0321, -0.0140,\n",
      "         -0.0415, -0.0910, -0.1060,  0.0707,  0.0914, -0.0913, -0.0930, -0.0859,\n",
      "          0.0180, -0.0357,  0.0544,  0.0224,  0.0031,  0.0878, -0.0012, -0.0688,\n",
      "         -0.0903,  0.0369, -0.0047,  0.0507, -0.0112, -0.0686,  0.0821,  0.0039,\n",
      "          0.0731, -0.0238, -0.0539,  0.1085,  0.0200, -0.0238, -0.0498, -0.0873,\n",
      "          0.0112,  0.0625, -0.0641,  0.0196, -0.0224,  0.0536,  0.0997,  0.0977,\n",
      "         -0.0325,  0.0339, -0.0896, -0.0691,  0.0996,  0.0057,  0.0944, -0.1002,\n",
      "         -0.0295,  0.0521,  0.0804, -0.0639,  0.0370, -0.0389, -0.0330, -0.0618,\n",
      "          0.1008,  0.0151,  0.0062,  0.0785, -0.0161, -0.0163, -0.0035, -0.0421,\n",
      "         -0.0854, -0.0651,  0.0811, -0.0546,  0.0962,  0.0869, -0.0772, -0.0716,\n",
      "          0.0146, -0.0919,  0.0037,  0.1037]], grad_fn=<SliceBackward>) \n",
      "\n",
      "Layer: fc3.bias | Size: torch.Size([10]) | Values : tensor([-0.0599,  0.0336], grad_fn=<SliceBackward>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view weights for each layer\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
