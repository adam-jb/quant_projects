{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "southwest-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Notes from weeks 3 and 4 of https://www.coursera.org/learn/convolutional-neural-networks/lecture/nEeJM/object-localization\n",
    "\n",
    "# to make: \n",
    "# apply pre-trained YOLO to a few family Australia photos\n",
    "# apply pre-trained U-Net to Australia photos\n",
    "# YOLO with transfer learning applied to some data (could it learn to identify faces? - it's only one class)\n",
    "# get pre-trained siamese for facial verification, and transfer with new photos of fam\n",
    "\n",
    "\n",
    "\n",
    "### to identify photos of the same thing from a large pool of photos:\n",
    "# 1) (maybe - this might eliminate useful information) use YOLO or similar to do object detection and crop image to\n",
    "# only have object of interest\n",
    "# (If don't crop image might at least want to make it so the object of interest is in the centre of the image\n",
    "# so that's consistent across all images: could use YOLO's output for that too)\n",
    "# 2) train siamese network to recognise when two images are of the same thing, and when they aren't\n",
    "#Â Usual things apply: training data of same distribution and balance as real-world data, etc\n",
    "# Ideally would have multiple images of each item from different angles\n",
    "# (could then make my own augmentations from there, eg: changing light/tone/rotation/mirror/etc of image)\n",
    "# If can't identify similar images with high confidence, could at least assign a probability\n",
    "# which could be combined with other information in Bayes Net to give a final score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# some views on training sizes needed per class:\n",
    "# https://stackoverflow.com/questions/55356982/how-many-imagesminimum-should-be-there-in-each-classes-for-training-yolo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "respective-street",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1\n",
    "# detection problem: where you have multiple objects in image and want to localise (get bounding box) for all of them\n",
    "\n",
    "\n",
    "# to get bounding box:\n",
    "# penultimate layer feeds into classification softmax layer, and also into a layer which \n",
    "# predicts a bounding box (or maybe combines them both, as per the vectors feeding into the loss func\n",
    "# defined below)\n",
    "\n",
    "# (0,0) is top left; (1,1) is bottom left\n",
    "# Bounding box prediction takes the form of 4 values:\n",
    "# bx, by (mid of rect coords); bw (width of rect); bh (height of rect)\n",
    "\n",
    "# Y (vector to predict) for single value with 4 possible classification values \n",
    "# (of which 1 is 'nothing there') has 8 values:\n",
    "# Pc (prob anything other than 'nothing' is there)\n",
    "# bx by bw bh\n",
    "# c1, c2, c3 (probs assigned to each of the not-nothing classifications)\n",
    "\n",
    "# if Pc = 'nothing there', then dont care about all other values. Think they aren't evaluated in the loss\n",
    "# function in this case (perhaps)\n",
    "\n",
    "def mse_loss_localisation_single_object(y_hat, y):\n",
    "    \"\"\"input two 1d numpy arrays\"\"\"\n",
    "    if y[0] == 1:           \n",
    "        return np.sum(np.power(y_hat - y, 2))  # if there is something in the image: is the classification right\n",
    "                                            # and how close is the bounding box\n",
    "    if y[0] == 0:\n",
    "        return np.power(y_hat[0] - y[0], 2)   # if nothing in image then only evaluate this \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "# landmark detection: return x and y coords of something interesting in an image\n",
    "# can modify NN to do that by adding two more values to output vector, presumably\n",
    "# with an extra value to tell you if the object of interest is present at all (eg: something\n",
    "# to tell you where nose and eyes are on pictures of people, and 1+ images have no faces in them\n",
    "# so add term for whether face is present or not)\n",
    "\n",
    "# landmarks can be defined on the face, say 64 key landmarks, which effectively become\n",
    "# features extracted from pictures of people's faces \n",
    "\n",
    "def landmark_feature_counter(landmarks_count):\n",
    "    return 2 * landmarks_count + 1   # the +1 is the indicator for whether obj of interest (eg face) is there at all\n",
    "\n",
    "# AR uses landmarks to work out how best to append objects to pictures of faces\n",
    "\n",
    "# landmark can also tell you people's actions or poses, by landmarking differnt parts of the body\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-navigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial training might have heavily cropped images\n",
    "\n",
    "# sliding windows: scanning across image several times over with gradually increasing window sizes, putting\n",
    "# each window image through a CNN which has been trained on heavily cropped images of the thing you're\n",
    "# interested in (eg: cars, where the car takes up almost the whole image)\n",
    "\n",
    "\n",
    "# Sliding windows can be sped up by implementing it \"convolutionally\". This means:\n",
    "# Converting all FC layers to 1x1 conv layers. This can be done without changing the operations done, only\n",
    "# the dimensions of tensors passed around\n",
    "# However this would still meaning passing the same number of bounding boxes through the network\n",
    "# So conv the whole image (or a wider part of it) and put it through the process, giving you\n",
    "# bigger layers for the conv layers which replace the FC ones , and each square in the tensor\n",
    "# gives you the info you'd have from a whole freeze-frame from a sliding window\n",
    "# eg if you were to slide in 2x2 windows, you'd end up with 2x2xC data\n",
    "# This saves some compute\n",
    "# It can be done in a bigger scale, eg: replacing 8x8 sliding window with one conv layer\n",
    "\n",
    "\n",
    "# see the 10:00 mark for diagram on how conv on a single freezeframe of a window can do the same thing\n",
    "# as lots of frozen windows using FC \n",
    "# https://www.coursera.org/learn/convolutional-neural-networks/lecture/6UnU4/convolutional-implementation-of-sliding-windows\n",
    "\n",
    "\n",
    "\n",
    "# 1x1 conv layers can be effectively do the same operations as FC layers, see here: \n",
    "# https://www.coursera.org/learn/convolutional-neural-networks/lecture/6UnU4/convolutional-implementation-of-sliding-windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the above, the bounding boxes won't be too accurate (output is just whether something is present in a given\n",
    "# sliding window). To improve this use Yolo:\n",
    "\n",
    "# divide image into grid (19x19 is common)\n",
    "# put each individual image through algo set out in Cell 1 above to detect class and bounding box\n",
    "# objects are assigned to the grid-image which contains it's midpoint\n",
    "# Works fine so long as no more than 1 object in each cell (or empty)\n",
    "\n",
    "# because it uses convolution to save iteratively moving over sliding windows it can be used for realtime\n",
    "# object detection\n",
    "\n",
    "# for bounding box: within each window, the top left is 0,0 the buttom right is 1,1 \n",
    "# bw or bh could be greater than 1 if the bounding box is bigger than the individual window (this happens)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-atlantic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_over_union(area_intersection, area_union):\n",
    "    \"\"\"intersection over union (IoU) for two bounding boxes\n",
    "    \n",
    "    Generally, if IoU > 0.5 the predicted bounding box will be judged close enough to the real bounding\n",
    "    box to be \"correct\" (could set higher than 0.5 threshold if you like)\n",
    "    \n",
    "    IoU can also be used as general metric of overlap between two boxes\n",
    "    \"\"\"\n",
    "    return area_intersection / area_union\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-trailer",
   "metadata": {},
   "outputs": [],
   "source": [
    "## non-max suppression: prevents you detecting the same object more than once\n",
    "\n",
    "# each window will give a probability of detecting an object, so where there's overlap (using IoU)\n",
    "# set probabilities to zero for all windows apart from the one with the highest probability\n",
    "\n",
    "### full algo:\n",
    "# 1) discard all probs with probability <= 0.6\n",
    "# 2) pick box with highest probability for that class\n",
    "# 3) discard any remaining boxes with IoU > 0.5 of this chosen box\n",
    "# So if IoU < 0.5, it will be detected as a separate instance of that object\n",
    "\n",
    "\n",
    "# non max supression: \n",
    "# https://www.coursera.org/learn/convolutional-neural-networks/lecture/dvrjH/non-max-suppression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Anchor boxes:\n",
    "\n",
    "# define rough shape you expect for a given class (eg tall rect for person, fat rect for car),\n",
    "# then output vector has usual_len (eg: 8 values if using example in Cell 1) * total_anchor_boxes (eg: 3 if \n",
    "# looking for 3 classes of object)\n",
    "# objects are assigned an anchor box by whichever anchor box best fits it (highest IoU), and\n",
    "# non-max suppression is only done for objects in the same assigned anchor box\n",
    "# this allows for overlapping objects of different types and shapes, such as a person standing\n",
    "# in front of a car\n",
    "\n",
    "\n",
    "# cant handle more objects in one grid cell than there are anchor boxes (doesnt happen much if you use 19x19 grid)\n",
    "\n",
    "\n",
    "# ways of chosing anchor boxes:\n",
    "# by judgement\n",
    "# k-means clustering of bounding boxes in real data to find archetypical anchor box shapes which \n",
    "# represent one or more classes\n",
    "\n",
    "\n",
    "# more on anchor boxes:\n",
    "# https://www.coursera.org/learn/convolutional-neural-networks/lecture/yNwO0/anchor-boxes\n",
    "\n",
    "\n",
    "\n",
    "# yolo is a FCN = fully convolutional network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amino-colombia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def len_of_output_vec(no_of_classes, no_of_anchor_boxes):\n",
    "    \"\"\"Guessing youd often have same number of anchor boxes as classes, but not always\"\"\"\n",
    "    return (5 + no_of_classes) * no_of_anchor_boxes\n",
    "\n",
    "\n",
    "# if you break grid into 19*19, and len_of_output_vec = 24, then output from conv net for one image would\n",
    "# be 19*19*24\n",
    "# or maybe 19*19*3*8 (if each of the 3 anchor boxes has its own array of len 8)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# summary of YOLO algo process:\n",
    "# https://www.coursera.org/learn/convolutional-neural-networks/lecture/fF3O0/yolo-algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-bulgarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R-CNN: Regional CNN\n",
    "# uses 'segmentation algorithm' to decide which regions are worth scanning, so not all image is processed\n",
    "# by CNN, but only those which seem interesting (saving computer on trying to classify bits of sky, for instance)\n",
    "\n",
    "# was fast, Fast R-CNN faster\n",
    "# Faster R-CNN uses conv. net to propose regions, making it faster, but YOLO is still quite a bit faster\n",
    "\n",
    "# Andrew Ng doesnt use R-CNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-genre",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic segmentation: aims to put exact outline around object to the exact pixel\n",
    "\n",
    "# U-net is an algo that labels every single pixel as either 'nothing' of one of the classes proposed\n",
    "\n",
    "# U-net architecture looks like CNN as dims reduce and channels increase, however instead of going through\n",
    "# FC midway, it reverses dimensional direction and starts reducing channels and increasing dimensions in \n",
    "# each convolution until it outputs a tensor of the same dimensions at the input image (which is necessary\n",
    "# as its putting a classification value to each pixel)\n",
    "\n",
    "# uses transpose convolutions to get from smaller matrix to larger one\n",
    "# transpose convolution uses a conv matrix larger than the input matrix, where each\n",
    "# value in the input matrix is, one at a time, multiplied by all values in \n",
    "# the convo matrix, the values from which then go to the output matrix, with the\n",
    "# transpose-conv output matrices from each individual input projected onto the \n",
    "# main output matrix in much the same way as a standard conv process scans: moving\n",
    "# along with a certain stride size. Where individual transposed conv. matrices overlap\n",
    "# the results are summed\n",
    "\n",
    "# summary of transpose conv process:\n",
    "# https://www.coursera.org/learn/convolutional-neural-networks/lecture/kyoqR/transpose-convolutions\n",
    "\n",
    "\n",
    "# U-net uses skip-connections: passing outputs from one of the first conv layers straight to its mirror \n",
    "# layer (in the sense it's the same dimensions) nearer the end of the network. This gives this late-layer\n",
    "# more high-res pixel-level info on where things are in the image which the data which has been through the\n",
    "# many convolutions won't have (as it's lost a certain amount of resolution in doing this)\n",
    "\n",
    "\n",
    "# there are multiple skip-connection links between layers in U-net\n",
    "\n",
    "# output for single image = original_image_width * original_image_height * number_of_classes\n",
    "\n",
    "\n",
    "\n",
    "# full u-net architecture overview:\n",
    "# https://www.coursera.org/learn/convolutional-neural-networks/lecture/GIIWY/u-net-architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-celebration",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Week 4!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italic-small",
   "metadata": {},
   "outputs": [],
   "source": [
    "# facial verification: have one image of face and have to determine if that is the face it claims to be\n",
    "            # (a 1:1 test)\n",
    "    \n",
    "# facial recognition: see if image of a face is anyone in a list of people (harder as much more room to make mistake)\n",
    "        # (a 1:k test)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "given-overview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# facial verification is a one-shot problem: learning to recognise the person from a single training image\n",
    "# \n",
    "\n",
    "\n",
    "# use a similarity function: calculates degree of difference between images im1 and im2\n",
    "# below a certain similarity score you say the person is who they claim to be\n",
    "\n",
    "\n",
    "# Siamese network: only one network really, which involves conv. then FC layers, ending in a \n",
    "# vector (length 128 in the video) from the input of a face. Then compute the similarity\n",
    "# of the two 128 vectors which represent two faces\n",
    "\n",
    "\n",
    "# When training siamese network: want to ensure that if two pictures are of the same person\n",
    "# the difference between the output vectors is small, and big if the people are different\n",
    "# This is done in the network \"DeepFace\"\n",
    "\n",
    "\n",
    "# \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-marine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet loss: taking reference image, then getting similarity score between it and:\n",
    "    # 1) another image of that person\n",
    "    # 2) image of someone else\n",
    "# want similarity to be higher for image of same person by a 'margin' parameter which we set\n",
    "    \n",
    "triplet_loss_single_trio_of_encoded_images(reference, same, different, margin):\n",
    "    \"\"\"\n",
    "    loss is 0 if difference between reference and 'same' is at least margin's value lower than\n",
    "    diff between reference and 'different', otherwise it's positive\n",
    "    \"\"\"\n",
    "    result = np.sum(np.abs(reference - same))**2 - np.sum(np.abs(reference - different))**2 + margin\n",
    "        ### CHECK the above: the sum of 128 values might come after this whole formula rather than be inside it\n",
    "    return np.max(result, 0)   \n",
    "\n",
    "\n",
    "# though the videos talk about one-shot learning, to train you need several pictures of people\n",
    "# so you can train the model\n",
    "\n",
    "# when making training set, pick images which are similar to the reference for the non-match person\n",
    "# so the model learns better distinctions between individual people\n",
    "# If you choose the non-match photo randomly, it will be too easy for the model, and it won't \n",
    "# learn much while getting very little loss on training\n",
    "\n",
    "\n",
    "    \n",
    "# more on triplet loss about halfway thru this vid:\n",
    "# https://www.coursera.org/learn/convolutional-neural-networks/lecture/HuUtN/triplet-loss\n",
    "\n",
    "\n",
    "\n",
    "# could put the element-wise difference between two encoded faces into a logistic regression to \n",
    "# predict if same face or not (binary prediction)\n",
    "# a couple of other tweaks one could make:\n",
    "# https://www.coursera.org/learn/convolutional-neural-networks/lecture/xTihv/face-verification-and-binary-classification\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respective-telephone",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-spare",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
